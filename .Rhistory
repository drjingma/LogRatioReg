print(paste0("mean prediction error: ", mean(slr.mse)))
print(paste0("standard deviation: ", (sd(slr.mse))))
print(paste0("standard error: ", (sd(slr.mse)) / rep.n))
print(paste0(
"95% CI: (",
mean(slr.mse) - 2 * (sd(slr.mse)) / rep.n,
", ",
mean(slr.mse) + 2 * (sd(slr.mse)) / rep.n, ")"
))
getwd()
# libraries
library(mvtnorm) # for rmvnorm if allow.noise in fitSLR()
library(limSolve) # for constrained lm, lsei()
library(stats) # for hclust()
library(balance) # for sbp.fromHclust()
# set up parallelization
library(doFuture)
library(parallel)
registerDoFuture()
nworkers = detectCores()
plan(multisession, workers = nworkers)
library(doRNG)
rng.seed = 123
registerDoRNG(rng.seed)
# Dr. Ma sources
source("RCode/func_libs.R")
# Kristyn sources
functions_path = "Kristyn/Functions/"
source(paste0(functions_path, "supervisedlogratios.R"))
# settings
tol = 1e-4
get_lambda = "glmnet"
# Cross-validation
cv.n_lambda = 100
cv.K = 10
# Repetitions
rep.n = 100
n.train = 70
n.test = 28
# data
# 98 samples, 87 genera
# replace zero counts with 0.5 (maximum rounding error)
load(paste0("Data/", "BMI.rda"))
# dim(raw_data) # 98 x 89
# dim(X) # 98 x 87
# dim(X.prop) # 98 x 87
log.X.prop = log(X.prop)
n = dim(X)[1]
num.genera = dim(X)[2]
b=1
j=1
# split into train and test sets
train.idx = sample(1:n, n.train)
Xtrain = X.prop[train.idx, ]
Ytrain = y[train.idx]
Xtest = X.prop[-train.idx, ]
Ytest = y[-train.idx]
# Fit Lasso on training set
cv.fits = cvSLR(y = Ytrain, X = Xtrain, nlam = cv.n_lambda, nfolds = cv.K,
get_lambda = get_lambda)
plot(cv.fits$lambda, cv.fits$cvm)
lambdamin = cv.fits$lambda[which.min(cv.fits$cvm)]
lambdamin
cv.fits$lambda
lambda_glmnet = lambdamin
get_lambda
get_lambda = "original"
# Fit Lasso on training set
cv.fits = cvSLR(y = Ytrain, X = Xtrain, nlam = cv.n_lambda, nfolds = cv.K,
get_lambda = get_lambda)
plot(cv.fits$lambda, cv.fits$cvm)
lambdamin = cv.fits$lambda[which.min(cv.fits$cvm)]
lambda_original = cv.fits$lambda[which.min(cv.fits$cvm)]
lambda_glmnet
lambda_original
################################################################################
# Not the same simulated data for classic & compositional lasso tests
# from LogRatioReg > RCode > sup-balances.R
################################################################################
# Supervised Log Ratios Regression
# do hierarchical clustering with specified linkage for compositional data X and y
getSupervisedTree = function(y, X, linkage, allow.noise = FALSE, noise){
# browser()
n = dim(X)[1]
p = dim(X)[2]
if(length(y) != n) stop("getSupervisedTree() error: dim(X)[1] != length(y)!")
# calculate correlation of each pair of log-ratios with response y
cormat = matrix(1, p, p) # diagonal == 1
y_demeaned = y - mean(y)
for (j in 1:(p - 1)){
for (k in (j + 1):p){
Zjk = log(X[, j]) - log(X[, k])
Zjk_demeaned = Zjk - mean(Zjk)
if(all(Zjk == 0)){ # add noise (hopefully only necessary if bootstrap!)
if(allow.noise) Zjk = rmvnorm(n, rep(0, n), noise * diag(n))
} else{
val = abs(cor(Zjk_demeaned, y_demeaned))
}
cormat[j, k] = val
cormat[k, j] = val
}
}
# find out which columns give na
# for(i in 1:p) if(!all(!is.na(cormat[, i]))) print(paste0(i, ":", which(is.na(cormat[, i]))))
#
# give the rows and columns the names of taxa in X,
#   so that sbp.fromHclust works later
rownames(cormat) = colnames(X)
rownames(cormat) = colnames(X)
# get dissimilarity matrix
Gammamat = 1 - abs(cormat)
# get tree from hierarchical clustering
btree_slr = hclust(as.dist(Gammamat), method = linkage)
return(btree_slr)
}
# using a hierarchical tree, compute the balances for X
computeBalances = function(X, btree){
# compute balances from hclust object using balance pkg:
# 1. build SBP (serial binary partition) matrix from hclust object
sbp = sbp.fromHclust(btree) # U = basis vectors
# 2. calculate balances from SBP matrix
balances = balance.fromSBP(X, sbp)
return(balances)
}
fitSLR = function(
y, X, linkage = "complete", lambda = NULL, nlam = 20, nfolds = 10,
get_lambda = NULL, allow.noise = FALSE, noise = 1e-12
){
n <- nrow(X)
p <- ncol(X)
# compute balances
btree = getSupervisedTree(y, X, linkage, allow.noise, noise)
Xb = computeBalances(X, btree)
# get lambda sequence, if not already given
if(is.null(lambda) & is.null(get_lambda)) get_lambda = "glmnet"
if(is.null(lambda)){ # get lambda
if(!is.null(get_lambda)){
if(get_lambda == "sup-balances" | get_lambda == "original" | get_lambda == 0){ # like in sup-balances.R
# apply to user-defined lambda sequence, if given. if not, let glmnet provide.
cv_exact = cv.glmnet(x = Xb, y = y, lambda = lambda, nlambda = nlam)
# get a new lambda sequence (why?)
lambda = log(cv_exact$lambda)
lambda = exp(seq(max(lambda), min(lambda) + 2, length.out = nlam))
} else if(get_lambda == "ConstrLasso" | get_lambda = "complasso" | get_lambda == 1){ # like ConstrLasso()
# get sequence of tuning parameter lambda
maxlam <- 2*max(abs(crossprod(Xb, y) / n))
lambda <- exp(seq(log(maxlam), log(1e-4), length.out = nlam))
} else if(get_lambda == "glmnet" | get_lambda == 2){ # like glmnet
lambda = NULL # lambda stays NULL
}
}
}
# fit lasso (using glmnet)
if(get_lambda == "glmnet" | get_lambda == 2){ # like glmnet
cv_exact = glmnet(x = Xb, y = y, lambda = lambda, nlambda = nlam)
} else{
cv_exact = glmnet(x = Xb, y = y, lambda = lambda)
}
return(list(
int = cv_exact$a0,
bet = cv_exact$beta,
lambda = cv_exact$lambda,
glmnet = cv_exact,
btree = btree
))
}
cvSLR = function(
y, X, linkage = "complete", lambda = NULL, nlam = 20, nfolds = 10,
get_lambda = NULL, allow.noise = FALSE, noise = 1e-12
){
n <- nrow(X)
p <- ncol(X)
# Fit to the original data
slr = fitSLR(y, X, linkage, lambda, nlam, get_lambda, allow.noise, noise)
bet = slr$bet
int = slr$int
lambda = slr$lambda
nlam = length(lambda)
btree = slr$btree
# Split the data into K folds
shuffle = sample(1:n)
idfold = (shuffle %% nfolds) + 1 # IDs for which fold each obs goes into
n_fold = as.vector(table(idfold)) # number of things in each fold
cvm = rep(NA, nlam) # want to have CV(lambda)
cvm_sqerror = matrix(rep(NA, nfolds * nlam), nfolds, nlam) # calculate squared error for each fold, needed for CV(lambda) calculation
cvse = rep(NA, nlam) # want to have SE_CV(lambda)
cvse_errorsd =  matrix(rep(NA, nfolds * nlam), nfolds, nlam) # calculate sd of error for each fold, needed for SE_CV(lambda) calculation
# Calculate Lasso for each fold removed
for (j in 1:nfolds){
# Training data
Xtrain = X[idfold != j, ]
Ytrain = y[idfold != j]
# Test data
Xtest = X[idfold == j, ]
Ytest = y[idfold == j]
# Calculate LASSO on that fold using fitLASSO
slr_j = fitSLR(Ytrain, Xtrain, linkage, lambda, nlam, nfolds, get_lambda, allow.noise, noise)
# Any additional calculations that are needed for calculating CV and SE_CV(lambda)
for(m in 1:nlam){
Ypred = slr_j$int[m] +
computeBalances(Xtest, btree) %*% as.matrix(slr_j$bet[ , m])
cvm_sqerror[j, m] = sum(crossprod(Ytest - Ypred))
cvse_errorsd[j, m] = cvm_sqerror[j, m] / n_fold[j]
}
}
# Calculate CV(lambda) and SE_CV(lambda) for each value of lambda
cvse = sqrt(apply(cvse_errorsd, 2, var)) / sqrt(nfolds)
cvm = colMeans(cvm_sqerror)
# Find lambda_min = argmin{CV(lambda)}
lambda_min_idx = which.min(cvm)
lambda_min = lambda[lambda_min_idx]
# Find lambda_1se = maximal lambda s.t. CV(lambda) <= CV(lambda_min) + CV_SE(lambda_min)
bound = cvm[lambda_min_idx] + cvse[lambda_min_idx]
lambda_1se_idx = which(cvm <= bound)
lambda_1se = lambda[min(lambda_1se_idx)]
return(list(
int = int,
bet = bet,
lambda = lambda,
btree = btree,
lambda_min = lambda_min,
lambda_1se = lambda_1se,
cvm = cvm,
cvm_idx = lambda_min_idx,
cvse = cvse,
cvse_idx = lambda_1se_idx
))
}
#
LRtoLC = function(
LRcoefficients, btree
){
U = sbp.fromHclust(btree) # matrix of basis vectors, p x (p - 1)
LCcoefficients = U %*% LRcoefficients
return(LCcoefficients)
}
get_lambda = "complasso"
# Fit Lasso on training set
cv.fits = cvSLR(y = Ytrain, X = Xtrain, nlam = cv.n_lambda, nfolds = cv.K,
get_lambda = get_lambda)
plot(x = cv.fits$lambda, y = cv.fits$cvm)
30.30 - 2 * 0.97
30.30 + 2 * 0.97
30.55 - 2 * 1.04
30.55 + 2 * 1.04
output_home = "Kristyn/Experiments/output/"
rng.seed = 123
rep.n = 100
################################################################################
# Compositional Lasso #
################################################################################
complasso = readRDS(paste0(
output_home,
"complasso_prediction",
"_seed", rng.seed,
".rds"
))
dim(complasso)
complasso.mse = colMeans(complasso)
print(paste0("mean prediction error: ", mean(complasso.mse)))
print(paste0("standard deviation: ", (sd(complasso.mse))))
print(paste0("standard error: ", (sd(complasso.mse)) / sqrt(rep.n)))
print(paste0(
"95% CI: (",
mean(complasso.mse) - 2 * (sd(complasso.mse)) / sqrt(rep.n),
", ",
mean(complasso.mse) + 2 * (sd(complasso.mse)) / sqrt(rep.n), ")"
))
get_lambda = "glmnet"
slr = readRDS(paste0(
output_home,
"slr_prediction",
"_getlambda", get_lambda,
"_seed", rng.seed,
".rds"
))
dim(slr)
slr.mse = colMeans(slr)
print(paste0("mean prediction error: ", mean(slr.mse)))
print(paste0("standard deviation: ", (sd(slr.mse))))
print(paste0("standard error: ", (sd(slr.mse)) / rep.n))
print(paste0(
"95% CI: (",
mean(slr.mse) - 2 * (sd(slr.mse)) / rep.n,
", ",
mean(slr.mse) + 2 * (sd(slr.mse)) / rep.n, ")"
))
dim(slr)
slr.mse = colMeans(slr)
print(paste0("mean prediction error: ", mean(slr.mse)))
print(paste0("standard deviation: ", (sd(slr.mse))))
print(paste0("standard error: ", (sd(slr.mse)) / sqrt(rep.n)))
print(paste0(
"95% CI: (",
mean(slr.mse) - 2 * (sd(slr.mse)) / sqrt(rep.n),
", ",
mean(slr.mse) + 2 * (sd(slr.mse)) / sqrt(rep.n), ")"
))
getwd()
# libraries
library(limSolve) # for constrained lm
library(glmnet)
# set up parallelization
library(doFuture)
library(parallel)
registerDoFuture()
nworkers = detectCores()
plan(multisession, workers = nworkers)
library(doRNG)
rng.seed = 123 # 123, 345
registerDoRNG(rng.seed)
# settings
tol = 1e-4
# Cross-validation
cv.n_lambda = 100
cv.K = 10
# Repetitions
rep.n = 100
n.train = 70
n.test = 28
# data
# 98 samples, 87 genera
# replace zero counts with 0.5 (maximum rounding error)
load(paste0("Data/", "BMI.rda"))
# dim(raw_data) # 98 x 89
# dim(X) # 98 x 87
# dim(X.prop) # 98 x 87
log.X.prop = log(X.prop)
n = dim(X)[1]
num.genera = dim(X)[2]
################################################################################
# Experiments
################################################################################
# They generate 100 bootstrap samples and use the same CV procedure to select
#   the genera (for stable selection results)
b=1
# split into train and test sets
train.idx = sample(1:n, n.train)
Xtrain = log.X.prop[train.idx, ]
Ytrain = y[train.idx]
Xtest = log.X.prop[-train.idx, ]
Ytest = y[-train.idx]
# refitted CV
# Split the data into 10 folds
cv.fits = glmnet(x = log.X.prop, y = y, nlambda = nlam)
cv.fits = glmnet(x = log.X.prop, y = y, nlambda = cv.n_lambda)
cv.fits = glmnet(x = log.X.prop, y = y, nlambda = cv.n_lambda, nfolds = cv.K)
# Fit Lasso on training set
cv.fits = glmnet(x = log.X.prop, y = y, nlambda = cv.n_lambda, nfolds = cv.K)
lambdamin.idx = which.min(cv.fits$cvm)
betahat = cv.fits$bet[, lambdamin.idx]
betahat = cv.fits$bet[, lambdamin.idx]
names(betahat) = colnames(betahat)
cv.fits$beta
betahat = as.matrix(cv.fits$beta[, lambdamin.idx])
betahat
betahat = as.matrix(cv.fits$beta)[, lambdamin.idx]
betahat
cv.fits$beta
dim(cv.fits$beta)
dim(log.X.prop)
lambdamin.idx
?cv.glmnet
# Fit Lasso on training set
cv.fits = cv.glmnet(x = log.X.prop, y = y, nlambda = cv.n_lambda, nfolds = cv.K)
lambdamin.idx = which.min(cv.fits$cvm)
betahat = as.matrix(cv.fits$beta)[, lambdamin.idx]
lambdamin.idx
cv.fits$lambda.min
cv.exact = glmnet(x = log.X.prop, y = y, lambda = cv.fits$lambda.min)
?glmnet
# Fit Lasso on training set
cv.fits = cv.glmnet(x = log.X.prop, y = y, nlambda = cv.n_lambda, nfolds = cv.K)
cv.exact = glmnet(x = log.X.prop, y = y, lambda = cv.fits$lambda.min)
betahat = as.matrix(cv.exact$beta)[, lambdamin.idx]
betahat = as.matrix(cv.exact$beta)
betahat
cv.exact$beta
# get fitted model (no refitting)
intercept = cv.exact$a0
betahat = as.matrix(cv.exact$beta)
names(betahat) = colnames(betahat)
# get fitted model (no refitting)
intercept = cv.exact$a0
predictCLM = function(X){
intercept + X %*% betahat
}
# calculate squared error (prediction error?)
Ypred = predictCLM(Xtest)
mse
getwd()
# libraries
library(limSolve) # for constrained lm
library(glmnet)
# set up parallelization
library(doFuture)
library(parallel)
registerDoFuture()
nworkers = detectCores()
plan(multisession, workers = nworkers)
library(doRNG)
rng.seed = 123 # 123, 345
registerDoRNG(rng.seed)
# settings
tol = 1e-4
# Cross-validation
cv.n_lambda = 100
cv.K = 10
# Repetitions
rep.n = 100
n.train = 70
n.test = 28
# data
# 98 samples, 87 genera
# replace zero counts with 0.5 (maximum rounding error)
load(paste0("Data/", "BMI.rda"))
# dim(raw_data) # 98 x 89
# dim(X) # 98 x 87
# dim(X.prop) # 98 x 87
log.X.prop = log(X.prop)
n = dim(X)[1]
num.genera = dim(X)[2]
################################################################################
# Experiments
################################################################################
# They generate 100 bootstrap samples and use the same CV procedure to select
#   the genera (for stable selection results)
pred.err = foreach(
b = 1:rep.n,
.combine = cbind#, .noexport = c("ConstrLassoC0", "ConstrLaso", "cv.func")
) %dopar% {
source("RCode/func_libs.R")
library(limSolve)
# split into train and test sets
train.idx = sample(1:n, n.train)
Xtrain = log.X.prop[train.idx, ]
Ytrain = y[train.idx]
Xtest = log.X.prop[-train.idx, ]
Ytest = y[-train.idx]
# refitted CV
# Split the data into 10 folds
# Fit Lasso on training set
cv.fits = cv.glmnet(x = log.X.prop, y = y, nlambda = cv.n_lambda, nfolds = cv.K)
cv.exact = glmnet(x = log.X.prop, y = y, lambda = cv.fits$lambda.min)
betahat = as.matrix(cv.exact$beta)
names(betahat) = colnames(betahat)
# get fitted model (no refitting)
intercept = cv.exact$a0
predictCLM = function(X){
intercept + X %*% betahat
}
# calculate squared error (prediction error?)
Ypred = predictCLM(Xtest)
(Ytest - Ypred)^2
}
dim(pred.err)
mse = colMeans(pred.err)
print(paste0("mean prediction error: ", mean(mse)))
print(paste0("standard deviation: ", (sd(mse))))
print(paste0("standard error: ", (sd(mse)) / sqrt(rep.n)))
print(paste0(
"95% CI: (",
mean(mse) - 2 * (sd(mse)) / sqrt(rep.n),
", ",
mean(mse) + 2 * (sd(mse)) / sqrt(rep.n), ")"
))
saveRDS(pred.err,
file = paste0("Kristyn/Experiments/output",
"/lasso_prediction",
"_seed", rng.seed,
".rds"))
################################################################################
# Lasso on Log Ratios #
################################################################################
lasso = readRDS(paste0(
output_home,
"lasso_prediction",
"_seed", rng.seed,
".rds"
))
output_home = "Kristyn/Experiments/output/"
rng.seed = 123
rep.n = 100
################################################################################
# Lasso on Log Ratios #
################################################################################
lasso = readRDS(paste0(
output_home,
"lasso_prediction",
"_seed", rng.seed,
".rds"
))
dim(lasso)
slr.mse = colMeans(lasso)
################################################################################
lasso = readRDS(paste0(
output_home,
"lasso_prediction",
"_seed", rng.seed,
".rds"
))
dim(lasso)
lasso.mse = colMeans(lasso)
print(paste0("mean prediction error: ", mean(lasso.mse)))
print(paste0("standard deviation: ", (sd(lasso.mse))))
print(paste0("standard error: ", (sd(lasso.mse)) / sqrt(rep.n)))
print(paste0(
"95% CI: (",
mean(lasso.mse) - 2 * (sd(lasso.mse)) / sqrt(rep.n),
", ",
mean(lasso.mse) + 2 * (sd(lasso.mse)) / sqrt(rep.n), ")"
))
