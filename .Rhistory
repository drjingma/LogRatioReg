# Calculate CV(lambda) for each value of lambda
cvm = (1 / n) * colSums(cvm_sqerror)
# Find lambda_min = argmin{CV(lambda)}
lambda_min_index = which.min(cvm)
lambda_min = Lasso_j$lambda_seq[lambda_min_index]
# then refit the model with this data.
source(paste0(functions_path, "compositional_lasso.R"))
# sourcing doesn't work, need to run the functions manually for some reason??
Lasso_final = fitCompositionalLASSO(X ,y, lambda_min)
matrix(1, 3, 3)
is.null(NA)
is.na(NA)
lambda_min[2]
workdir = "/home/kristyn/Documents/research/supervisedlogratios/LogRatioReg"
setwd(workdir)
# libraries
library(mvtnorm)
library(balance)
library(selbal)
library(propr)
library(microbenchmark)
library(ggplot2)
library(logratiolasso) # bates & tibshirani 2019
image_path = "/home/kristyn/Pictures"
# Dr. Ma sources
source("RCode/func_libs.R")
source("COAT-master/coat.R")
# Kristyn sources
functions_path = "Kristyn/Functions/"
source(paste0(functions_path, "classic_lasso.R"))
source(paste0(functions_path, "compositional_lasso.R"))
source(paste0(functions_path, "supervisedlogratios.R"))
source(paste0(functions_path, "coat.R"))
source(paste0(functions_path, "principlebalances.R"))
source(paste0(functions_path, "propr.R"))
source(paste0(functions_path, "selbal.R"))
# settings
cv.seed = 123
cv.n_lambda = 50
# data
# 98 samples, 87 genera
# replace zero counts with 0.5 (maximum rounding error)
DataFolder <- "/Data/"
load(paste0(workdir, DataFolder, "BMI.rda"))
# dim(raw_data) # 98 x 89
# dim(X) # 98 x 87
# dim(X.prop) # 98 x 87
n = dim(X)[1]
num.genera = dim(X)[2]
################################################################################
# Choosing the tuning parameter -- an example of what goes on in the bootstrap #
################################################################################
# Lin et. al. 2014 applied the proposed method using a refitted version of
#   ten-fold cross-validation to choose the tuning parameter, where
#   the prediction of each sample split is computed with the refitted coeffs
#   (obtained after model selection and without penalization)
# Split the data into 10 folds
cv.K = 10
set.seed(cv.seed)
shuffle = sample(1:n)
idfold = (shuffle %% cv.K) + 1
n_fold = as.vector(table(idfold))
# Do cross-validation
# calculate squared error (prediction error?) for each fold,
#   needed for CV(lambda) calculation
cvm = rep(NA, cv.n_lambda) # want to have CV(lambda)
cvm_sqerror = matrix(NA, cv.K, cv.n_lambda)
# Fit Lasso for each fold removed
for (j in 1:cv.K){
# Training data
Xtrain = X[idfold != j, ]
Ytrain = y[idfold != j]
# Test data
Xtest = X[idfold == j, ]
Ytest = y[idfold == j]
# Fit LASSO on that fold using fitLASSOcompositional
# first, take out columns that have all 0.5's, because they shouldn't be selected anyway (and lead to problems)
cols.0.5 = apply(Xtrain, 2, FUN = function(vec) all(vec == 0.5))
Xtrain = Xtrain[, !cols.0.5]
Xtest = Xtest[, !cols.0.5]
XYdata = data.frame(Xtrain, y = Ytrain)
Lasso_j = fitCompositionalLASSO(Xtrain ,Ytrain) # a problem in centering and scaling X cols with all 0.5's
non0.betas = Lasso_j$beta_mat != 0 # diff lambda = diff col
for(m in 1:cv.n_lambda){
# get refitted coefficients, after model selection and w/o penalization
selected_variables = non0.betas[, m]
if(all(!selected_variables)){ # if none selected
refit = lm(y ~ 1, data = XYdata)
} else{ # otherwise, fit on selected variables
refit = lm(
as.formula(paste("y", "~",
paste(colnames(XYdata)[which(selected_variables)],
collapse = "+"),
sep = "")),
data=XYdata)
}
# calculate squared error (prediction error?)
# if(!all.equal(colnames(Xtest), colnames(Xtrain))) stop("Xtrain and Xtest don't have the same variable names")
newx = Xtest[, selected_variables, drop = FALSE]
Ypred = predict(refit, newdata = data.frame(newx), type = "response")
cvm_sqerror[j, m] = sum(crossprod(Ytest - Ypred))
}
}
# Calculate CV(lambda) for each value of lambda
cvm = (1 / n) * colSums(cvm_sqerror)
# Find lambda_min = argmin{CV(lambda)}
lambda_min_index = which.min(cvm)
lambda_min = Lasso_j$lambda_seq[lambda_min_index]
# then refit the model with this data.
source(paste0(functions_path, "compositional_lasso.R"))
# sourcing doesn't work, need to run the functions manually for some reason??
Lasso_final = fitCompositionalLASSO(X ,y, lambda_min)
workdir = "/home/kristyn/Documents/research/supervisedlogratios/LogRatioReg"
setwd(workdir)
# libraries
library(mvtnorm)
library(balance)
library(selbal)
library(propr)
library(microbenchmark)
library(ggplot2)
library(logratiolasso) # bates & tibshirani 2019
image_path = "/home/kristyn/Pictures"
# Dr. Ma sources
source("RCode/func_libs.R")
source("COAT-master/coat.R")
# Kristyn sources
functions_path = "Kristyn/Functions/"
source(paste0(functions_path, "classic_lasso.R"))
source(paste0(functions_path, "compositional_lasso.R"))
source(paste0(functions_path, "supervisedlogratios.R"))
source(paste0(functions_path, "coat.R"))
source(paste0(functions_path, "principlebalances.R"))
source(paste0(functions_path, "propr.R"))
source(paste0(functions_path, "selbal.R"))
# settings
cv.seed = 123
cv.n_lambda = 50
# data
# 98 samples, 87 genera
# replace zero counts with 0.5 (maximum rounding error)
DataFolder <- "/Data/"
load(paste0(workdir, DataFolder, "BMI.rda"))
# dim(raw_data) # 98 x 89
# dim(X) # 98 x 87
# dim(X.prop) # 98 x 87
n = dim(X)[1]
num.genera = dim(X)[2]
################################################################################
# Choosing the tuning parameter -- an example of what goes on in the bootstrap #
################################################################################
# Lin et. al. 2014 applied the proposed method using a refitted version of
#   ten-fold cross-validation to choose the tuning parameter, where
#   the prediction of each sample split is computed with the refitted coeffs
#   (obtained after model selection and without penalization)
# Split the data into 10 folds
cv.K = 10
set.seed(cv.seed)
shuffle = sample(1:n)
idfold = (shuffle %% cv.K) + 1
n_fold = as.vector(table(idfold))
# Do cross-validation
# calculate squared error (prediction error?) for each fold,
#   needed for CV(lambda) calculation
cvm = rep(NA, cv.n_lambda) # want to have CV(lambda)
cvm_sqerror = matrix(NA, cv.K, cv.n_lambda)
# Fit Lasso for each fold removed
for (j in 1:cv.K){
# Training data
Xtrain = X[idfold != j, ]
Ytrain = y[idfold != j]
# Test data
Xtest = X[idfold == j, ]
Ytest = y[idfold == j]
# Fit LASSO on that fold using fitLASSOcompositional
# first, take out columns that have all 0.5's, because they shouldn't be selected anyway (and lead to problems)
cols.0.5 = apply(Xtrain, 2, FUN = function(vec) all(vec == 0.5))
Xtrain = Xtrain[, !cols.0.5]
Xtest = Xtest[, !cols.0.5]
XYdata = data.frame(Xtrain, y = Ytrain)
Lasso_j = fitCompositionalLASSO(Xtrain ,Ytrain) # a problem in centering and scaling X cols with all 0.5's
non0.betas = Lasso_j$beta_mat != 0 # diff lambda = diff col
for(m in 1:cv.n_lambda){
# get refitted coefficients, after model selection and w/o penalization
selected_variables = non0.betas[, m]
if(all(!selected_variables)){ # if none selected
refit = lm(y ~ 1, data = XYdata)
} else{ # otherwise, fit on selected variables
refit = lm(
as.formula(paste("y", "~",
paste(colnames(XYdata)[which(selected_variables)],
collapse = "+"),
sep = "")),
data=XYdata)
}
# calculate squared error (prediction error?)
# if(!all.equal(colnames(Xtest), colnames(Xtrain))) stop("Xtrain and Xtest don't have the same variable names")
newx = Xtest[, selected_variables, drop = FALSE]
Ypred = predict(refit, newdata = data.frame(newx), type = "response")
cvm_sqerror[j, m] = sum(crossprod(Ytest - Ypred))
}
}
# Calculate CV(lambda) for each value of lambda
cvm = (1 / n) * colSums(cvm_sqerror)
# Find lambda_min = argmin{CV(lambda)}
lambda_min_index = which.min(cvm)
lambda_min = Lasso_j$lambda_seq[lambda_min_index]
# then refit the model with this data.
source(paste0(functions_path, "compositional_lasso.R"))
# sourcing doesn't work, need to run the functions manually for some reason??
Lasso_final = fitCompositionalLASSO(X ,y, lambda_min)
if(NA < 0) 2
if(NULL < 0) 2
na.omit(c(NA, 1, 2))
?na.omit
remove.na
remove.na()
workdir = "/home/kristyn/Documents/research/supervisedlogratios/LogRatioReg"
setwd(workdir)
# libraries
library(mvtnorm)
library(balance)
library(selbal)
library(propr)
library(microbenchmark)
library(ggplot2)
library(logratiolasso) # bates & tibshirani 2019
image_path = "/home/kristyn/Pictures"
# Dr. Ma sources
source("RCode/func_libs.R")
source("COAT-master/coat.R")
# Kristyn sources
functions_path = "Kristyn/Functions/"
source(paste0(functions_path, "classic_lasso.R"))
source(paste0(functions_path, "compositional_lasso.R"))
source(paste0(functions_path, "supervisedlogratios.R"))
source(paste0(functions_path, "coat.R"))
source(paste0(functions_path, "principlebalances.R"))
source(paste0(functions_path, "propr.R"))
source(paste0(functions_path, "selbal.R"))
# settings
cv.seed = 123
cv.n_lambda = 50
# data
# 98 samples, 87 genera
# replace zero counts with 0.5 (maximum rounding error)
DataFolder <- "/Data/"
load(paste0(workdir, DataFolder, "BMI.rda"))
# dim(raw_data) # 98 x 89
# dim(X) # 98 x 87
# dim(X.prop) # 98 x 87
n = dim(X)[1]
num.genera = dim(X)[2]
################################################################################
# Choosing the tuning parameter -- an example of what goes on in the bootstrap #
################################################################################
# Lin et. al. 2014 applied the proposed method using a refitted version of
#   ten-fold cross-validation to choose the tuning parameter, where
#   the prediction of each sample split is computed with the refitted coeffs
#   (obtained after model selection and without penalization)
# Split the data into 10 folds
cv.K = 10
set.seed(cv.seed)
shuffle = sample(1:n)
idfold = (shuffle %% cv.K) + 1
n_fold = as.vector(table(idfold))
# Do cross-validation
# calculate squared error (prediction error?) for each fold,
#   needed for CV(lambda) calculation
cvm = rep(NA, cv.n_lambda) # want to have CV(lambda)
cvm_sqerror = matrix(NA, cv.K, cv.n_lambda)
# Fit Lasso for each fold removed
for (j in 1:cv.K){
# Training data
Xtrain = X[idfold != j, ]
Ytrain = y[idfold != j]
# Test data
Xtest = X[idfold == j, ]
Ytest = y[idfold == j]
# Fit LASSO on that fold using fitLASSOcompositional
# first, take out columns that have all 0.5's, because they shouldn't be selected anyway (and lead to problems)
cols.0.5 = apply(Xtrain, 2, FUN = function(vec) all(vec == 0.5))
Xtrain = Xtrain[, !cols.0.5]
Xtest = Xtest[, !cols.0.5]
XYdata = data.frame(Xtrain, y = Ytrain)
Lasso_j = fitCompositionalLASSO(Xtrain ,Ytrain) # a problem in centering and scaling X cols with all 0.5's
non0.betas = Lasso_j$beta_mat != 0 # diff lambda = diff col
for(m in 1:cv.n_lambda){
# get refitted coefficients, after model selection and w/o penalization
selected_variables = non0.betas[, m]
if(all(!selected_variables)){ # if none selected
refit = lm(y ~ 1, data = XYdata)
} else{ # otherwise, fit on selected variables
refit = lm(
as.formula(paste("y", "~",
paste(colnames(XYdata)[which(selected_variables)],
collapse = "+"),
sep = "")),
data=XYdata)
}
# calculate squared error (prediction error?)
# if(!all.equal(colnames(Xtest), colnames(Xtrain))) stop("Xtrain and Xtest don't have the same variable names")
newx = Xtest[, selected_variables, drop = FALSE]
Ypred = predict(refit, newdata = data.frame(newx), type = "response")
cvm_sqerror[j, m] = sum(crossprod(Ytest - Ypred))
}
}
# Calculate CV(lambda) for each value of lambda
cvm = (1 / n) * colSums(cvm_sqerror)
# Find lambda_min = argmin{CV(lambda)}
lambda_min_index = which.min(cvm)
lambda_min = Lasso_j$lambda_seq[lambda_min_index]
# then refit the model with this data.
source(paste0(functions_path, "compositional_lasso.R"))
# sourcing doesn't work, need to run the functions manually for some reason??
Lasso_final = fitCompositionalLASSO(X ,y, lambda_min)
workdir = "/home/kristyn/Documents/research/supervisedlogratios/LogRatioReg"
setwd(workdir)
# libraries
library(mvtnorm)
library(balance)
library(selbal)
library(propr)
library(microbenchmark)
library(ggplot2)
library(logratiolasso) # bates & tibshirani 2019
image_path = "/home/kristyn/Pictures"
# Dr. Ma sources
source("RCode/func_libs.R")
source("COAT-master/coat.R")
# Kristyn sources
functions_path = "Kristyn/Functions/"
source(paste0(functions_path, "classic_lasso.R"))
source(paste0(functions_path, "compositional_lasso.R"))
source(paste0(functions_path, "supervisedlogratios.R"))
source(paste0(functions_path, "coat.R"))
source(paste0(functions_path, "principlebalances.R"))
source(paste0(functions_path, "propr.R"))
source(paste0(functions_path, "selbal.R"))
# settings
cv.seed = 123
cv.n_lambda = 50
# data
# 98 samples, 87 genera
# replace zero counts with 0.5 (maximum rounding error)
DataFolder <- "/Data/"
load(paste0(workdir, DataFolder, "BMI.rda"))
# dim(raw_data) # 98 x 89
# dim(X) # 98 x 87
# dim(X.prop) # 98 x 87
n = dim(X)[1]
num.genera = dim(X)[2]
################################################################################
# Choosing the tuning parameter -- an example of what goes on in the bootstrap #
################################################################################
# Lin et. al. 2014 applied the proposed method using a refitted version of
#   ten-fold cross-validation to choose the tuning parameter, where
#   the prediction of each sample split is computed with the refitted coeffs
#   (obtained after model selection and without penalization)
# Split the data into 10 folds
cv.K = 10
set.seed(cv.seed)
shuffle = sample(1:n)
idfold = (shuffle %% cv.K) + 1
n_fold = as.vector(table(idfold))
# Do cross-validation
# calculate squared error (prediction error?) for each fold,
#   needed for CV(lambda) calculation
cvm = rep(NA, cv.n_lambda) # want to have CV(lambda)
cvm_sqerror = matrix(NA, cv.K, cv.n_lambda)
# Fit Lasso for each fold removed
for (j in 1:cv.K){
# Training data
Xtrain = X[idfold != j, ]
Ytrain = y[idfold != j]
# Test data
Xtest = X[idfold == j, ]
Ytest = y[idfold == j]
# Fit LASSO on that fold using fitLASSOcompositional
# first, take out columns that have all 0.5's, because they shouldn't be selected anyway (and lead to problems)
cols.0.5 = apply(Xtrain, 2, FUN = function(vec) all(vec == 0.5))
Xtrain = Xtrain[, !cols.0.5]
Xtest = Xtest[, !cols.0.5]
XYdata = data.frame(Xtrain, y = Ytrain)
Lasso_j = fitCompositionalLASSO(Xtrain ,Ytrain) # a problem in centering and scaling X cols with all 0.5's
non0.betas = Lasso_j$beta_mat != 0 # diff lambda = diff col
for(m in 1:cv.n_lambda){
# get refitted coefficients, after model selection and w/o penalization
selected_variables = non0.betas[, m]
if(all(!selected_variables)){ # if none selected
refit = lm(y ~ 1, data = XYdata)
} else{ # otherwise, fit on selected variables
refit = lm(
as.formula(paste("y", "~",
paste(colnames(XYdata)[which(selected_variables)],
collapse = "+"),
sep = "")),
data=XYdata)
}
# calculate squared error (prediction error?)
# if(!all.equal(colnames(Xtest), colnames(Xtrain))) stop("Xtrain and Xtest don't have the same variable names")
newx = Xtest[, selected_variables, drop = FALSE]
Ypred = predict(refit, newdata = data.frame(newx), type = "response")
cvm_sqerror[j, m] = sum(crossprod(Ytest - Ypred))
}
}
# Calculate CV(lambda) for each value of lambda
cvm = (1 / n) * colSums(cvm_sqerror)
# Find lambda_min = argmin{CV(lambda)}
lambda_min_index = which.min(cvm)
lambda_min = Lasso_j$lambda_seq[lambda_min_index]
# then refit the model with this data.
# sourcing doesn't work, need to run the functions manually for some reason??
Lasso_final = fitCompositionalLASSO(X ,y, lambda_min)
workdir = "/home/kristyn/Documents/research/supervisedlogratios/LogRatioReg"
setwd(workdir)
# libraries
library(mvtnorm)
library(balance)
library(selbal)
library(propr)
library(microbenchmark)
library(ggplot2)
library(logratiolasso) # bates & tibshirani 2019
image_path = "/home/kristyn/Pictures"
# Dr. Ma sources
source("RCode/func_libs.R")
source("COAT-master/coat.R")
# Kristyn sources
functions_path = "Kristyn/Functions/"
source(paste0(functions_path, "classic_lasso.R"))
source(paste0(functions_path, "compositional_lasso.R"), echo = TRUE)
source(paste0(functions_path, "supervisedlogratios.R"))
source(paste0(functions_path, "coat.R"))
source(paste0(functions_path, "principlebalances.R"))
source(paste0(functions_path, "propr.R"))
source(paste0(functions_path, "selbal.R"))
# settings
cv.seed = 123
cv.n_lambda = 50
# data
# 98 samples, 87 genera
# replace zero counts with 0.5 (maximum rounding error)
DataFolder <- "/Data/"
load(paste0(workdir, DataFolder, "BMI.rda"))
# dim(raw_data) # 98 x 89
# dim(X) # 98 x 87
# dim(X.prop) # 98 x 87
n = dim(X)[1]
num.genera = dim(X)[2]
################################################################################
# Choosing the tuning parameter -- an example of what goes on in the bootstrap #
################################################################################
# Lin et. al. 2014 applied the proposed method using a refitted version of
#   ten-fold cross-validation to choose the tuning parameter, where
#   the prediction of each sample split is computed with the refitted coeffs
#   (obtained after model selection and without penalization)
# Split the data into 10 folds
cv.K = 10
set.seed(cv.seed)
shuffle = sample(1:n)
idfold = (shuffle %% cv.K) + 1
n_fold = as.vector(table(idfold))
# Do cross-validation
# calculate squared error (prediction error?) for each fold,
#   needed for CV(lambda) calculation
cvm = rep(NA, cv.n_lambda) # want to have CV(lambda)
cvm_sqerror = matrix(NA, cv.K, cv.n_lambda)
# Fit Lasso for each fold removed
for (j in 1:cv.K){
# Training data
Xtrain = X[idfold != j, ]
Ytrain = y[idfold != j]
# Test data
Xtest = X[idfold == j, ]
Ytest = y[idfold == j]
# Fit LASSO on that fold using fitLASSOcompositional
# first, take out columns that have all 0.5's, because they shouldn't be selected anyway (and lead to problems)
cols.0.5 = apply(Xtrain, 2, FUN = function(vec) all(vec == 0.5))
Xtrain = Xtrain[, !cols.0.5]
Xtest = Xtest[, !cols.0.5]
XYdata = data.frame(Xtrain, y = Ytrain)
Lasso_j = fitCompositionalLASSO(Xtrain ,Ytrain) # a problem in centering and scaling X cols with all 0.5's
non0.betas = Lasso_j$beta_mat != 0 # diff lambda = diff col
for(m in 1:cv.n_lambda){
# get refitted coefficients, after model selection and w/o penalization
selected_variables = non0.betas[, m]
if(all(!selected_variables)){ # if none selected
refit = lm(y ~ 1, data = XYdata)
} else{ # otherwise, fit on selected variables
refit = lm(
as.formula(paste("y", "~",
paste(colnames(XYdata)[which(selected_variables)],
collapse = "+"),
sep = "")),
data=XYdata)
}
# calculate squared error (prediction error?)
# if(!all.equal(colnames(Xtest), colnames(Xtrain))) stop("Xtrain and Xtest don't have the same variable names")
newx = Xtest[, selected_variables, drop = FALSE]
Ypred = predict(refit, newdata = data.frame(newx), type = "response")
cvm_sqerror[j, m] = sum(crossprod(Ytest - Ypred))
}
}
# Calculate CV(lambda) for each value of lambda
cvm = (1 / n) * colSums(cvm_sqerror)
# Find lambda_min = argmin{CV(lambda)}
lambda_min_index = which.min(cvm)
lambda_min = Lasso_j$lambda_seq[lambda_min_index]
# then refit the model with this data.
# sourcing doesn't work, need to run the functions manually for some reason??
Lasso_final = fitCompositionalLASSO(X ,y, lambda_min)
