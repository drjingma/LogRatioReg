selected_variables = non0.betas
if(all(!selected_variables)){ # if none selected
finalfit.bs = lm(y ~ 1, data = XYdata)
} else{ # otherwise, fit on selected variables
finalfit.bs = lm(
as.formula(paste("y", "~",
paste(colnames(XYdata)[which(selected_variables)],
collapse = "+"),
sep = "")),
data=XYdata)
}
# save the final fit for this bootstrap
bs.finalfits[[b]] = finalfit.bs
# record which variables were selected in this fit
bs.selected_variables[, b] = selected_variables
}
bs.selected_variables_numeric = apply(bs.selected_variables, 2, as.numeric)
bs.selection_percentages = apply(bs.selected_variables_numeric, 1, sum)
names(bs.selection_percentages) = rownames(bs.selected_variables)
bs.results = list(
seed = bs.seed,
finalfits = bs.finalfits,
selected_variables = bs.selected_variables,
selection_percentages = bs.selection_percentages
)
saveRDS(bs.results, file = "lin_bootstrap_results1.rds")
bs.selection_percentages[which(bs.selection_percentages >= 70)]
sort(bs.selection_percentages)
final.selected = colnames(log.X.prop)[which(bs.selection_percentages >= 50)]
bs.selection_percentages[which(bs.selection_percentages >= 70)]
finalfit
workdir = "/home/kristyn/Documents/research/supervisedlogratios/LogRatioReg"
setwd(workdir)
# libraries
library(mvtnorm)
library(balance)
library(selbal)
library(propr)
library(microbenchmark)
library(ggplot2)
library(logratiolasso) # bates & tibshirani 2019
image_path = "/home/kristyn/Pictures"
# Dr. Ma sources
source("RCode/func_libs.R")
source("COAT-master/coat.R")
# Kristyn sources
functions_path = "Kristyn/Functions/"
source(paste0(functions_path, "classic_lasso.R"))
source(paste0(functions_path, "compositional_lasso.R"))
source(paste0(functions_path, "supervisedlogratios.R"))
source(paste0(functions_path, "coat.R"))
source(paste0(functions_path, "principlebalances.R"))
source(paste0(functions_path, "propr.R"))
source(paste0(functions_path, "selbal.R"))
# settings
# Cross-validation
cv.seed = 1234
cv.n_lambda = 100
# Bootstrap
bs.seed = 1
bs.n = 100
# data
# 98 samples, 87 genera
# replace zero counts with 0.5 (maximum rounding error)
DataFolder <- "/Data/"
load(paste0(workdir, DataFolder, "BMI.rda"))
# dim(raw_data) # 98 x 89
# dim(X) # 98 x 87
# dim(X.prop) # 98 x 87
log.X.prop = log(X.prop)
n = dim(X)[1]
num.genera = dim(X)[2]
################################################################################
# Choosing the tuning parameter -- an example of what goes on in the bootstrap #
################################################################################
# Lin et. al. 2014 applied the proposed method using a refitted version of
#   ten-fold cross-validation to choose the tuning parameter, where
#   the prediction of each sample split is computed with the refitted coeffs
#   (obtained after model selection and without penalization)
# Split the data into 10 folds
cv.K = 10
set.seed(cv.seed)
shuffle = sample(1:n)
idfold = (shuffle %% cv.K) + 1
n_fold = as.vector(table(idfold))
# Do cross-validation
# calculate squared error (prediction error?) for each fold,
#   needed for CV(lambda) calculation
cvm = rep(NA, cv.n_lambda) # want to have CV(lambda)
cvm_sqerror = matrix(NA, cv.K, cv.n_lambda)
# Fit Lasso for each fold removed
for (j in 1:cv.K){
# Training data
Xtrain = log.X.prop[idfold != j, ]
Ytrain = y[idfold != j]
# Test data
Xtest = log.X.prop[idfold == j, ]
Ytest = y[idfold == j]
# Fit LASSO on that fold using fitLASSOcompositional
# first, take out columns that have all 0.5's, because they shouldn't be selected anyway (and lead to problems)
# cols.0.5 = apply(Xtrain, 2, FUN = function(vec) all(vec == 0.5))
# print(any(cols.0.5))
# Xtrain = Xtrain[, !cols.0.5]
# Xtest = Xtest[, !cols.0.5]
print(paste0("for fold j, genera with sd = 0 are: ", which(apply(Xtrain, 2, sd) == 0)))
}
?sort
workdir = "/home/kristyn/Documents/research/supervisedlogratios/LogRatioReg"
setwd(workdir)
# libraries
library(mvtnorm)
library(balance)
library(selbal)
library(propr)
library(microbenchmark)
library(ggplot2)
library(logratiolasso) # bates & tibshirani 2019
image_path = "/home/kristyn/Pictures"
# Dr. Ma sources
source("RCode/func_libs.R")
source("COAT-master/coat.R")
# Kristyn sources
functions_path = "Kristyn/Functions/"
source(paste0(functions_path, "classic_lasso.R"))
source(paste0(functions_path, "compositional_lasso.R"))
source(paste0(functions_path, "supervisedlogratios.R"))
source(paste0(functions_path, "coat.R"))
source(paste0(functions_path, "principlebalances.R"))
source(paste0(functions_path, "propr.R"))
source(paste0(functions_path, "selbal.R"))
# settings
# Cross-validation
cv.seed = 1234
cv.n_lambda = 100
# Bootstrap
bs.seed = 1
bs.n = 100
# data
# 98 samples, 87 genera
# replace zero counts with 0.5 (maximum rounding error)
DataFolder <- "/Data/"
load(paste0(workdir, DataFolder, "BMI.rda"))
# dim(raw_data) # 98 x 89
# dim(X) # 98 x 87
# dim(X.prop) # 98 x 87
log.X.prop = log(X.prop)
n = dim(X)[1]
num.genera = dim(X)[2]
################################################################################
# Choosing the tuning parameter -- an example of what goes on in the bootstrap #
################################################################################
# Lin et. al. 2014 applied the proposed method using a refitted version of
#   ten-fold cross-validation to choose the tuning parameter, where
#   the prediction of each sample split is computed with the refitted coeffs
#   (obtained after model selection and without penalization)
# Split the data into 10 folds
cv.K = 10
set.seed(cv.seed)
shuffle = sample(1:n)
idfold = (shuffle %% cv.K) + 1
n_fold = as.vector(table(idfold))
# Do cross-validation
# calculate squared error (prediction error?) for each fold,
#   needed for CV(lambda) calculation
cvm = rep(NA, cv.n_lambda) # want to have CV(lambda)
cvm_sqerror = matrix(NA, cv.K, cv.n_lambda)
# Fit Lasso for each fold removed
for (j in 1:cv.K){
# Training data
Xtrain = log.X.prop[idfold != j, ]
Ytrain = y[idfold != j]
# Test data
Xtest = log.X.prop[idfold == j, ]
Ytest = y[idfold == j]
# Fit LASSO on that fold using fitLASSOcompositional
# first, take out columns that have all 0.5's, because they shouldn't be selected anyway (and lead to problems)
# cols.0.5 = apply(Xtrain, 2, FUN = function(vec) all(vec == 0.5))
# print(any(cols.0.5))
# Xtrain = Xtrain[, !cols.0.5]
# Xtest = Xtest[, !cols.0.5]
print(paste0("for fold", j, ", genera with sd = 0 are: ", sort(apply(Xtrain, 2, sd))))
}
workdir = "/home/kristyn/Documents/research/supervisedlogratios/LogRatioReg"
setwd(workdir)
# libraries
library(mvtnorm)
library(balance)
library(selbal)
library(propr)
library(microbenchmark)
library(ggplot2)
library(logratiolasso) # bates & tibshirani 2019
image_path = "/home/kristyn/Pictures"
# Dr. Ma sources
source("RCode/func_libs.R")
source("COAT-master/coat.R")
# Kristyn sources
functions_path = "Kristyn/Functions/"
source(paste0(functions_path, "classic_lasso.R"))
source(paste0(functions_path, "compositional_lasso.R"))
source(paste0(functions_path, "supervisedlogratios.R"))
source(paste0(functions_path, "coat.R"))
source(paste0(functions_path, "principlebalances.R"))
source(paste0(functions_path, "propr.R"))
source(paste0(functions_path, "selbal.R"))
# settings
# Cross-validation
cv.seed = 1234
cv.n_lambda = 100
# Bootstrap
bs.seed = 1
bs.n = 100
# data
# 98 samples, 87 genera
# replace zero counts with 0.5 (maximum rounding error)
DataFolder <- "/Data/"
load(paste0(workdir, DataFolder, "BMI.rda"))
# dim(raw_data) # 98 x 89
# dim(X) # 98 x 87
# dim(X.prop) # 98 x 87
log.X.prop = log(X.prop)
n = dim(X)[1]
num.genera = dim(X)[2]
################################################################################
# Choosing the tuning parameter -- an example of what goes on in the bootstrap #
################################################################################
# Lin et. al. 2014 applied the proposed method using a refitted version of
#   ten-fold cross-validation to choose the tuning parameter, where
#   the prediction of each sample split is computed with the refitted coeffs
#   (obtained after model selection and without penalization)
# Split the data into 10 folds
cv.K = 10
set.seed(cv.seed)
shuffle = sample(1:n)
idfold = (shuffle %% cv.K) + 1
n_fold = as.vector(table(idfold))
# Do cross-validation
# calculate squared error (prediction error?) for each fold,
#   needed for CV(lambda) calculation
cvm = rep(NA, cv.n_lambda) # want to have CV(lambda)
cvm_sqerror = matrix(NA, cv.K, cv.n_lambda)
# Fit Lasso for each fold removed
for (j in 1:cv.K){
# Training data
Xtrain = log.X.prop[idfold != j, ]
Ytrain = y[idfold != j]
# Test data
Xtest = log.X.prop[idfold == j, ]
Ytest = y[idfold == j]
# Fit LASSO on that fold using fitLASSOcompositional
# first, take out columns that have all 0.5's, because they shouldn't be selected anyway (and lead to problems)
# cols.0.5 = apply(Xtrain, 2, FUN = function(vec) all(vec == 0.5))
# print(any(cols.0.5))
# Xtrain = Xtrain[, !cols.0.5]
# Xtest = Xtest[, !cols.0.5]
print(paste0("for fold", j, ", genera with sd = 0 are: ", sort(apply(Xtrain, 2, sd))[1]))
}
workdir = "/home/kristyn/Documents/research/supervisedlogratios/LogRatioReg"
setwd(workdir)
# libraries
library(mvtnorm)
library(balance)
library(selbal)
library(propr)
library(microbenchmark)
library(ggplot2)
library(logratiolasso) # bates & tibshirani 2019
image_path = "/home/kristyn/Pictures"
# Dr. Ma sources
source("RCode/func_libs.R")
source("COAT-master/coat.R")
# Kristyn sources
functions_path = "Kristyn/Functions/"
source(paste0(functions_path, "classic_lasso.R"))
source(paste0(functions_path, "compositional_lasso.R"))
source(paste0(functions_path, "supervisedlogratios.R"))
source(paste0(functions_path, "coat.R"))
source(paste0(functions_path, "principlebalances.R"))
source(paste0(functions_path, "propr.R"))
source(paste0(functions_path, "selbal.R"))
# settings
# Cross-validation
cv.seed = 1234
cv.n_lambda = 100
# Bootstrap
bs.seed = 1
bs.n = 100
# data
# 98 samples, 87 genera
# replace zero counts with 0.5 (maximum rounding error)
DataFolder <- "/Data/"
load(paste0(workdir, DataFolder, "BMI.rda"))
# dim(raw_data) # 98 x 89
# dim(X) # 98 x 87
# dim(X.prop) # 98 x 87
log.X.prop = log(X.prop)
n = dim(X)[1]
num.genera = dim(X)[2]
################################################################################
# Choosing the tuning parameter -- an example of what goes on in the bootstrap #
################################################################################
# Lin et. al. 2014 applied the proposed method using a refitted version of
#   ten-fold cross-validation to choose the tuning parameter, where
#   the prediction of each sample split is computed with the refitted coeffs
#   (obtained after model selection and without penalization)
# Split the data into 10 folds
cv.K = 10
set.seed(cv.seed)
shuffle = sample(1:n)
idfold = (shuffle %% cv.K) + 1
n_fold = as.vector(table(idfold))
# Do cross-validation
# calculate squared error (prediction error?) for each fold,
#   needed for CV(lambda) calculation
cvm = rep(NA, cv.n_lambda) # want to have CV(lambda)
cvm_sqerror = matrix(NA, cv.K, cv.n_lambda)
# Fit Lasso for each fold removed
for (j in 1:cv.K){
# Training data
Xtrain = log.X.prop[idfold != j, ]
Ytrain = y[idfold != j]
# Test data
Xtest = log.X.prop[idfold == j, ]
Ytest = y[idfold == j]
# Fit LASSO on that fold using fitLASSOcompositional
# first, take out columns that have all 0.5's, because they shouldn't be selected anyway (and lead to problems)
# cols.0.5 = apply(Xtrain, 2, FUN = function(vec) all(vec == 0.5))
# print(any(cols.0.5))
# Xtrain = Xtrain[, !cols.0.5]
# Xtest = Xtest[, !cols.0.5]
print(paste0("for fold", j, ", genera with sd = 0 are: ", sort(apply(Xtrain, 2, mean))[1]))
}
log(0.5)
workdir = "/home/kristyn/Documents/research/supervisedlogratios/LogRatioReg"
setwd(workdir)
# libraries
library(mvtnorm)
library(balance)
library(selbal)
library(propr)
library(microbenchmark)
library(ggplot2)
library(logratiolasso) # bates & tibshirani 2019
image_path = "/home/kristyn/Pictures"
# Dr. Ma sources
source("RCode/func_libs.R")
source("COAT-master/coat.R")
# Kristyn sources
functions_path = "Kristyn/Functions/"
source(paste0(functions_path, "classic_lasso.R"))
source(paste0(functions_path, "compositional_lasso.R"))
source(paste0(functions_path, "supervisedlogratios.R"))
source(paste0(functions_path, "coat.R"))
source(paste0(functions_path, "principlebalances.R"))
source(paste0(functions_path, "propr.R"))
source(paste0(functions_path, "selbal.R"))
# settings
# Cross-validation
cv.seed = 1234
cv.n_lambda = 100
# Bootstrap
bs.seed = 1
bs.n = 100
# data
# 98 samples, 87 genera
# replace zero counts with 0.5 (maximum rounding error)
DataFolder <- "/Data/"
load(paste0(workdir, DataFolder, "BMI.rda"))
# dim(raw_data) # 98 x 89
# dim(X) # 98 x 87
# dim(X.prop) # 98 x 87
log.X.prop = log(X.prop)
n = dim(X)[1]
num.genera = dim(X)[2]
################################################################################
# Choosing the tuning parameter -- an example of what goes on in the bootstrap #
################################################################################
# Lin et. al. 2014 applied the proposed method using a refitted version of
#   ten-fold cross-validation to choose the tuning parameter, where
#   the prediction of each sample split is computed with the refitted coeffs
#   (obtained after model selection and without penalization)
# Split the data into 10 folds
cv.K = 10
set.seed(cv.seed)
shuffle = sample(1:n)
idfold = (shuffle %% cv.K) + 1
n_fold = as.vector(table(idfold))
# Do cross-validation
# calculate squared error (prediction error?) for each fold,
#   needed for CV(lambda) calculation
cvm = rep(NA, cv.n_lambda) # want to have CV(lambda)
cvm_sqerror = matrix(NA, cv.K, cv.n_lambda)
# Fit Lasso for each fold removed
for (j in 1:cv.K){
# Training data
Xtrain = X.prop[idfold != j, ]
Ytrain = y[idfold != j]
# Test data
Xtest = log.X.prop[idfold == j, ]
Ytest = y[idfold == j]
# Fit LASSO on that fold using fitLASSOcompositional
# first, take out columns that have all 0.5's, because they shouldn't be selected anyway (and lead to problems)
# cols.0.5 = apply(Xtrain, 2, FUN = function(vec) all(vec == 0.5))
# print(any(cols.0.5))
# Xtrain = Xtrain[, !cols.0.5]
# Xtest = Xtest[, !cols.0.5]
XYdata = data.frame(Xtrain, y = Ytrain)
Lasso_j = fitCompositionalLASSO(Xtrain ,Ytrain, n_lambda = cv.n_lambda) # a problem in centering and scaling X cols with all 0.5's
}
workdir = "/home/kristyn/Documents/research/supervisedlogratios/LogRatioReg"
setwd(workdir)
# libraries
library(mvtnorm)
library(balance)
library(selbal)
library(propr)
library(microbenchmark)
library(ggplot2)
library(logratiolasso) # bates & tibshirani 2019
image_path = "/home/kristyn/Pictures"
# Dr. Ma sources
source("RCode/func_libs.R")
source("COAT-master/coat.R")
# Kristyn sources
functions_path = "Kristyn/Functions/"
source(paste0(functions_path, "classic_lasso.R"))
source(paste0(functions_path, "compositional_lasso.R"))
source(paste0(functions_path, "supervisedlogratios.R"))
source(paste0(functions_path, "coat.R"))
source(paste0(functions_path, "principlebalances.R"))
source(paste0(functions_path, "propr.R"))
source(paste0(functions_path, "selbal.R"))
# settings
# Cross-validation
cv.seed = 1234
cv.n_lambda = 100
# Bootstrap
bs.seed = 1
bs.n = 100
# data
# 98 samples, 87 genera
# replace zero counts with 0.5 (maximum rounding error)
DataFolder <- "/Data/"
load(paste0(workdir, DataFolder, "BMI.rda"))
# dim(raw_data) # 98 x 89
# dim(X) # 98 x 87
# dim(X.prop) # 98 x 87
log.X.prop = log(X.prop)
n = dim(X)[1]
num.genera = dim(X)[2]
################################################################################
# Choosing the tuning parameter -- an example of what goes on in the bootstrap #
################################################################################
# Lin et. al. 2014 applied the proposed method using a refitted version of
#   ten-fold cross-validation to choose the tuning parameter, where
#   the prediction of each sample split is computed with the refitted coeffs
#   (obtained after model selection and without penalization)
# Split the data into 10 folds
cv.K = 10
set.seed(cv.seed)
shuffle = sample(1:n)
idfold = (shuffle %% cv.K) + 1
n_fold = as.vector(table(idfold))
# Do cross-validation
# calculate squared error (prediction error?) for each fold,
#   needed for CV(lambda) calculation
cvm = rep(NA, cv.n_lambda) # want to have CV(lambda)
cvm_sqerror = matrix(NA, cv.K, cv.n_lambda)
# Fit Lasso for each fold removed
for (j in 1:cv.K){
# Training data
Xtrain = X.prop[idfold != j, ]
Ytrain = y[idfold != j]
# Test data
Xtest = log.X.prop[idfold == j, ]
Ytest = y[idfold == j]
# Fit LASSO on that fold using fitLASSOcompositional
# first, take out columns that have all 0.5's, because they shouldn't be selected anyway (and lead to problems)
cols.0.5 = apply(Xtrain, 2, FUN = function(vec) all(vec == 0.5))
print(paste0("for cv fold ", j, ", any 0.5 cols : ", any(cols.0.5)))
# print(any(cols.0.5))
# Xtrain = Xtrain[, !cols.0.5]
# Xtest = Xtest[, !cols.0.5]
XYdata = data.frame(Xtrain, y = Ytrain)
Lasso_j = fitCompositionalLASSO(Xtrain ,Ytrain, n_lambda = cv.n_lambda) # a problem in centering and scaling X cols with all 0.5's
}
X.prop[idfold != j, ]
View(X.prop)
View(X)
sort(bs.selection_percentages)
final.lm
bs.selection_percentages[which(bs.selection_percentages >= 70)]
sort(bs.selection_percentages)
final.selected = colnames(log.X.prop)[which(bs.selection_percentages >= 50)]
final.selected = c(
"Bacteria.Bacteroidetes.Bacteroidia.Bacteroidales.Rikenellaceae.Alistipes",
"Bacteria.Firmicutes.Clostridia.Clostridiales.Clostridiaceae.Clostridium",
"Bacteria.Firmicutes.Clostridia.Clostridiales.Veillonellaceae.Acidaminococcus",
"Bacteria.Firmicutes.Clostridia.Clostridiales.Veillonellaceae.Allisonella")
final.data = data.frame(log.X.prop[, final.selected], y)
final.lm = lm(y ~ ., final.data)
final.lm
final.lm.noint = lm(y ~ -1 + ., final.data)
final.lm.noint
final.lm
final.lm.noint2 = lm(y ~ -1 + ., scale(final.data, center = apply(final.data, 2, mean), scale = apply(final.data, 2, sd)))
scale(final.data, center = apply(final.data, 2, mean), scale = apply(final.data, 2, sd))
final.lm.noint2 = lm(y ~ -1 + ., as.matrix(scale(final.data, center = apply(final.data, 2, mean), scale = apply(final.data, 2, sd))))
final.lm.noint2 = lm(y ~ -1 + ., as.data.frame(scale(final.data, center = apply(final.data, 2, mean), scale = apply(final.data, 2, sd))))
final.lm.noint2
final.lm.noint2 = lm(y ~ ., as.data.frame(scale(final.data, center = apply(final.data, 2, mean), scale = apply(final.data, 2, sd))))
final.lm.noint2
