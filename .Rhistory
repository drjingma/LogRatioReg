# off-diagonal = cor(Zjk, y)
Zjk_demeaned = Zjk - mean(Zjk)
cormat[j, k] = cor(Zjk_demeaned, y_demeaned)
}
}
}
# give the rows and columns the names of taxa in X,
#   so that sbp.fromHclust works later
rownames(cormat) = colnames(X)
rownames(cormat) = colnames(X)
# get dissimilarity matrix
Gammamat = 1 - abs(cormat)
# get tree from hierarchical clustering
btree_slr = hclust(as.dist(Gammamat), method = linkage)
return(btree_slr)
}
# using a hierarchical tree, compute the balances for X
computeBalances = function(btree_slr, X){
# compute balances from hclust object using balance pkg:
# 1. build SBP (serial binary partition) matrix from hclust object
sbp_slr = sbp.fromHclust(btree_slr)
# 2. calculate balances from SBP matrix
balances_slr = balance.fromSBP(X, sbp_slr)
return(balances_slr)
}
fitGlmnet = function(X, y, lambda){
cv_exact = cv.glmnet(x = X, y = y, lambda = lambda)
lambda = log(cv_exact$lambda)
lambda_new = exp(seq(max(lambda), min(lambda) + 2, length.out = 100))
cv_exact2 = cv.glmnet(x = X, y = y, lambda = lambda_new)
refit_exact = glmnet(x = X,y = y, family = 'gaussian', lambda = cv_exact2$lambda.min)
return(list(cv.glmnet = cv_exact2, glmnet = refit_exact, beta = refit_exact$beta, lambda = refit_exact$lambda))
}
# fit SLR given data (X, y), linkage, and (optional) lambda
# this version is the clean version
fitSLRLasso = function(X, y, linkage, lambda = NULL, allow.noise = FALSE){
btree = getSupervisedTree(X, y, linkage, allow.noise)
Xb = computeBalances(btree, X)
# lasso fit
glm.temp = fitGlmnet(Xb, y, lambda)
return(list(
glm = glm.temp,
btree = btree,
betahat = as.vector(glm.temp$beta),
lambda = glm.temp$lamdba
))
}
# tried using the fitting function in Dr. Ma's code, but changed my mind
fitSLRLasso0 = function(X, y, linkage, lambda = NULL, allow.noise = TRUE){
# divide dataset into training and validation set
#   (dataset might be a training set being further divided for validation)
# n = dim(X)[1]
# shuffle = sample(1:n)
# # id_tv = (shuffle %% 2) + 1
# id_tv = (shuffle %% 3) + 1
# n_tv = as.vector(table(id_tv))
# Xtrain.tmp = X[id_tv %in% c(1, 2), ]
# Xvalid.tmp = X[id_tv == 3, ]
# ytrain.tmp = y[id_tv %in% c(1, 2)]
# yvalid.tmp = y[id_tv == 3]
# # fit
btree = getSupervisedTree(X, y, linkage, allow.noise)
# Xtrainb = computeBalances(btree, Xtrain.tmp)
# Xvalidb = computeBalances(btree, Xvalid.tmp)
Xb = computeBalances(btree, X)
# lasso fit
glm.temp = fitGlmnet(Xb, y, lambda)
return(list(
btree = btree,
betahat = as.vector(glm.temp$beta),
lambda = glm.temp$lamdba
))
}
fitCOATLasso = function(X, y, linkage, lambda = NULL){
# get COAT tree
d_coat = 1 - coat(X)$corr
rownames(d_coat) = colnames(X)
colnames(d_coat) = colnames(X)
coat_tree = hclust(as.dist(d_coat),method = linkage)
# compute balances
sbp_coat = sbp.fromHclust(coat_tree)
Xb = balance.fromSBP(X, sbp_coat)
# lasso fit
glm.temp = fitGlmnet(Xb, y, lambda)
return(list(
glm = glm.temp,
btree = coat_tree,
betahat = as.vector(glm.temp$beta),
lambda = glm.temp$lamdba
))
}
test_slrLASSO = fitSLRLasso(X.prop.tr, y.tr, linkage = "complete") # works now!
# this is why we need to take out the rare OTUs -- so that there is lower chance
# of splitting train and test set and ending up with 0 columns in train set
# fit to test data
slrLassofit = function(x){
# names(x) = test_slrLASSO$btree$labels
xb = computeBalances(test_slrLASSO$btree, x)
xb %*% test_slrLASSO$betahat
}
slrLassoyhat = slrLassofit(X.prop.te)
slrLassoMSE = mean(slrLassoyhat - y.te)^2
slrLassoMSE2 = predict(test_slrLASSO$glm, newx = data.matrix(X.prop.te))
# plot dendrogram
slr_btree = test_slrLASSO$btree
slr_btree$labels = 1:length(slr_btree$labels)
plot(slr_btree)
### trying coat
test_coatLASSO = fitCOATLasso(X.prop.tr, y.tr, linkage = "complete")
coatLassofit = function(x){
# names(x) = test_slrLASSO$btree$labels
xb = computeBalances(test_coatLASSO$btree, x)
xb %*% test_coatLASSO$betahat
}
coatLassoyhat = coatLassofit(X.prop.te)
coatLassoMSE = mean(coatLassoyhat - y.te)^2
# plot dendrogram
coat_btree = test_coatLASSO$btree
coat_btree$labels = 1:length(coat_btree$labels)
plot(coat_btree)
# trying principle balances
test_pbLASSO = fitPBLasso(X.prop.tr, y.tr, lambda = NULL)
# seeing proportion of 0 beta elts
comp.0beta = sum(betahatcompositional == 0) / length(betahatcompositional)
classic.0beta = sum(betahatclassic == 0) / length(betahatclassic)
slr.0beta = sum(test_slrLASSO$betahat == 0) / length(test_slrLASSO$betahat)
coat.0beta = sum(test_coatLASSO$betahat == 0) / length(test_coatLASSO$betahat)
compare.0beta = data.frame(compositionalLasso = comp.0beta,
classicLasso = classic.0beta,
coatLasso = coat.0beta,
slrLasso = slr.0beta)
compare.0beta
# compare MSEs of three methods
compare.mse = data.frame(compositionalLasso = compositionalLassoMSE,
classicLasso = classicLassoMSE,
coatLasso = coatLassoMSE,
slrLasso = slrLassoMSE)
compare.mse
slrLassoMSE2 = predict(test_slrLASSO$glm$cv.glmnet, newx = data.matrix(X.prop.te), type = "response", s = "lambda.min")
slrLassoMSE2 = predict(test_slrLASSO$glm$glmnet, newx = data.matrix(X.prop.te), type = "response", s = "lambda.min")
slrLassoMSE2 = predict(test_slrLASSO$glm$glmnet, newx = data.matrix(X.prop.te), type = "response", s = test_slrLASSO$lambda)
test_slrLASSO$lambda
coefficients(test_slrLASSO$glm$glmnet)
coefficients(test_slrLASSO$glm$glmnet)
slrLassoMSE2 = predict(glmfit0, newx = data.matrix(X.prop.te), type = "response", s = test_slrLASSO$lambda)
glmfit0 = test_slrLASSO$glm$glmnet
slrLassoMSE2 = predict(glmfit0, newx = data.matrix(X.prop.te), type = "response", s = test_slrLASSO$lambda)
glmfit0
glmfit0$beta
glmfit0$a0
as.matrix(X.prop.te)
as.matrix(X.prop.te, names = NULL)
as.matrix(X.prop.te, colnames = NULL)
X.prop.te2 = X.prop.te
colnames(X.prop.te2) = NULL
slrLassoMSE2 = predict(glmfit0, newx = X.prop.te2, type = "response", s = test_slrLASSO$lambda)
paste("z", 1:dim(X.prop.te2), sep = "")
paste("z", 1:dim(X.prop.te2)[2], sep = "")
colnames(X.prop.te2) = paste("z", 1:(dim(X.prop.te2)[2] - 1), sep = "")
colnames(X.prop.te2) = paste("z", 1:(dim(X.prop.te2)[2]), sep = "")
slrLassoMSE2 = predict(glmfit0, newx = X.prop.te2, type = "response", s = test_slrLASSO$lambda)
predict(test_slrLASSO, newx = xb, type = "response")
# fit to test data
slrLassofit = function(x){
# names(x) = test_slrLASSO$btree$labels
xb = computeBalances(test_slrLASSO$btree, x)
predict(test_slrLASSO, newx = xb, type = "response")
# xb %*% test_slrLASSO$betahat
}
slrLassoyhat = slrLassofit(X.prop.te)
# fit to test data
slrLassofit = function(x){
# names(x) = test_slrLASSO$btree$labels
xb = computeBalances(test_slrLASSO$btree, x)
predict(test_slrLASSO$glm, newx = xb, type = "response")
# xb %*% test_slrLASSO$betahat
}
slrLassoyhat = slrLassofit(X.prop.te)
predict(test_slrLASSO$glm$glmnet, newx = xb, type = "response")
# fit to test data
slrLassofit = function(x){
# names(x) = test_slrLASSO$btree$labels
xb = computeBalances(test_slrLASSO$btree, x)
predict(test_slrLASSO$glm$glmnet, newx = xb, type = "response")
# xb %*% test_slrLASSO$betahat
}
slrLassoyhat = slrLassofit(X.prop.te)
slrLassoMSE = mean(slrLassoyhat - y.te)^2
slrLassoMSE
# fit to test data
slrLassofit = function(x){
# names(x) = test_slrLASSO$btree$labels
xb = computeBalances(test_slrLASSO$btree, x)
predict(test_slrLASSO$glm, newx = xb, type = "response")
}
slrLassoyhat = slrLassofit(X.prop.te)
slrLassoMSE = mean(slrLassoyhat - y.te)^2
predict(test_slrLASSO$glmnet, newx = xb, type = "response")
# fit to test data
slrLassofit = function(x){
# names(x) = test_slrLASSO$btree$labels
xb = computeBalances(test_slrLASSO$btree, x)
predict(test_slrLASSO$glmnet, newx = xb, type = "response")
}
slrLassoyhat = slrLassofit(X.prop.te)
test_slrLASSO$glmnet
test_slrLASSO = fitSLRLasso(X.prop.tr, y.tr, linkage = "complete") # works now!
# fit to test data
slrLassofit = function(x){
# names(x) = test_slrLASSO$btree$labels
xb = computeBalances(test_slrLASSO$btree, x)
predict(test_slrLASSO$glmnet, newx = xb, type = "response")
}
slrLassoyhat = slrLassofit(X.prop.te)
# fit to test data
slrLassofit = function(x){
# names(x) = test_slrLASSO$btree$labels
xb = computeBalances(test_slrLASSO$btree, x)
predict(test_slrLASSO$glm, newx = xb, type = "response")
}
slrLassoyhat = slrLassofit(X.prop.te)
################################################################################
# Not the same simulated data for classic & compositional lasso tests
# from LogRatioReg > RCode > sup-balances.R
################################################################################
# Supervised Log Ratios Regression
# do hierarchical clustering with specified linkage for data X and y
getSupervisedTree = function(X, y, linkage, allow.noise = FALSE){
# browser()
n = dim(X)[1]
p = dim(X)[2]
if(length(y) != n) stop("getSupervisedTree() error: dim(X)[1] != length(y)!")
# calculate correlation of each pair of log-ratios with response y
cormat = matrix(NA, p, p)
y_demeaned = y - mean(y)
for(j in 1:p){
for(k in 1:p){
Zjk = log(X[, j]) - log(X[, k])
# diagonal = 1
if(j == k){
cormat[j, k] = 1
} else{
if(all(Zjk == 0)){
if(allow.noise){
Zjk = rnorm(n, 0, 1e-10) # add noise if Zjk = 0vec
} else{
print(j)
print(k)
print(Zjk)
stop(paste0("Zjk == 0vec for j = ", j, " and k = ", k ," leads to correlation NA"))
}
}
# off-diagonal = cor(Zjk, y)
Zjk_demeaned = Zjk - mean(Zjk)
cormat[j, k] = cor(Zjk_demeaned, y_demeaned)
}
}
}
# give the rows and columns the names of taxa in X,
#   so that sbp.fromHclust works later
rownames(cormat) = colnames(X)
rownames(cormat) = colnames(X)
# get dissimilarity matrix
Gammamat = 1 - abs(cormat)
# get tree from hierarchical clustering
btree_slr = hclust(as.dist(Gammamat), method = linkage)
return(btree_slr)
}
# using a hierarchical tree, compute the balances for X
computeBalances = function(btree_slr, X){
# compute balances from hclust object using balance pkg:
# 1. build SBP (serial binary partition) matrix from hclust object
sbp_slr = sbp.fromHclust(btree_slr)
# 2. calculate balances from SBP matrix
balances_slr = balance.fromSBP(X, sbp_slr)
return(balances_slr)
}
fitGlmnet = function(X, y, lambda){
cv_exact = cv.glmnet(x = X, y = y, lambda = lambda)
lambda = log(cv_exact$lambda)
lambda_new = exp(seq(max(lambda), min(lambda) + 2, length.out = 100))
cv_exact2 = cv.glmnet(x = X, y = y, lambda = lambda_new)
refit_exact = glmnet(x = X,y = y, family = 'gaussian', lambda = cv_exact2$lambda.min)
return(list(glmnet = refit_exact, beta = refit_exact$beta, lambda = refit_exact$lambda))
}
# fit SLR given data (X, y), linkage, and (optional) lambda
# this version is the clean version
fitSLRLasso = function(X, y, linkage, lambda = NULL, allow.noise = FALSE){
btree = getSupervisedTree(X, y, linkage, allow.noise)
Xb = computeBalances(btree, X)
# lasso fit
glm.temp = fitGlmnet(Xb, y, lambda)
return(list(
glm = glm.temp$glmnet,
btree = btree,
betahat = as.vector(glm.temp$beta),
lambda = glm.temp$lamdba
))
}
# tried using the fitting function in Dr. Ma's code, but changed my mind
fitSLRLasso0 = function(X, y, linkage, lambda = NULL, allow.noise = TRUE){
# divide dataset into training and validation set
#   (dataset might be a training set being further divided for validation)
# n = dim(X)[1]
# shuffle = sample(1:n)
# # id_tv = (shuffle %% 2) + 1
# id_tv = (shuffle %% 3) + 1
# n_tv = as.vector(table(id_tv))
# Xtrain.tmp = X[id_tv %in% c(1, 2), ]
# Xvalid.tmp = X[id_tv == 3, ]
# ytrain.tmp = y[id_tv %in% c(1, 2)]
# yvalid.tmp = y[id_tv == 3]
# # fit
btree = getSupervisedTree(X, y, linkage, allow.noise)
# Xtrainb = computeBalances(btree, Xtrain.tmp)
# Xvalidb = computeBalances(btree, Xvalid.tmp)
Xb = computeBalances(btree, X)
# lasso fit
glm.temp = fitGlmnet(Xb, y, lambda)
return(list(
btree = btree,
betahat = as.vector(glm.temp$beta),
lambda = glm.temp$lamdba
))
}
################################################################################
# Not the same simulated data for classic & compositional lasso tests
# from LogRatioReg > RCode > sup-balances.R
################################################################################
# Supervised Log Ratios Regression
# do hierarchical clustering with specified linkage for data X and y
getSupervisedTree = function(X, y, linkage, allow.noise = FALSE){
# browser()
n = dim(X)[1]
p = dim(X)[2]
if(length(y) != n) stop("getSupervisedTree() error: dim(X)[1] != length(y)!")
# calculate correlation of each pair of log-ratios with response y
cormat = matrix(NA, p, p)
y_demeaned = y - mean(y)
for(j in 1:p){
for(k in 1:p){
Zjk = log(X[, j]) - log(X[, k])
# diagonal = 1
if(j == k){
cormat[j, k] = 1
} else{
if(all(Zjk == 0)){
if(allow.noise){
Zjk = rnorm(n, 0, 1e-10) # add noise if Zjk = 0vec
} else{
print(j)
print(k)
print(Zjk)
stop(paste0("Zjk == 0vec for j = ", j, " and k = ", k ," leads to correlation NA"))
}
}
# off-diagonal = cor(Zjk, y)
Zjk_demeaned = Zjk - mean(Zjk)
cormat[j, k] = cor(Zjk_demeaned, y_demeaned)
}
}
}
# give the rows and columns the names of taxa in X,
#   so that sbp.fromHclust works later
rownames(cormat) = colnames(X)
rownames(cormat) = colnames(X)
# get dissimilarity matrix
Gammamat = 1 - abs(cormat)
# get tree from hierarchical clustering
btree_slr = hclust(as.dist(Gammamat), method = linkage)
return(btree_slr)
}
# using a hierarchical tree, compute the balances for X
computeBalances = function(btree_slr, X){
# compute balances from hclust object using balance pkg:
# 1. build SBP (serial binary partition) matrix from hclust object
sbp_slr = sbp.fromHclust(btree_slr)
# 2. calculate balances from SBP matrix
balances_slr = balance.fromSBP(X, sbp_slr)
return(balances_slr)
}
fitGlmnet = function(X, y, lambda){
cv_exact = cv.glmnet(x = X, y = y, lambda = lambda)
lambda = log(cv_exact$lambda)
lambda_new = exp(seq(max(lambda), min(lambda) + 2, length.out = 100))
cv_exact2 = cv.glmnet(x = X, y = y, lambda = lambda_new)
refit_exact = glmnet(x = X,y = y, family = 'gaussian', lambda = cv_exact2$lambda.min)
return(list(glmnet = refit_exact, beta = refit_exact$beta, lambda = refit_exact$lambda))
}
# fit SLR given data (X, y), linkage, and (optional) lambda
# this version is the clean version
fitSLRLasso = function(X, y, linkage, lambda = NULL, allow.noise = FALSE){
btree = getSupervisedTree(X, y, linkage, allow.noise)
Xb = computeBalances(btree, X)
# lasso fit
glm.temp = fitGlmnet(Xb, y, lambda)
return(list(
glmnet = glm.temp$glmnet,
btree = btree,
betahat = as.vector(glm.temp$beta),
lambda = glm.temp$lamdba
))
}
# tried using the fitting function in Dr. Ma's code, but changed my mind
fitSLRLasso0 = function(X, y, linkage, lambda = NULL, allow.noise = TRUE){
# divide dataset into training and validation set
#   (dataset might be a training set being further divided for validation)
# n = dim(X)[1]
# shuffle = sample(1:n)
# # id_tv = (shuffle %% 2) + 1
# id_tv = (shuffle %% 3) + 1
# n_tv = as.vector(table(id_tv))
# Xtrain.tmp = X[id_tv %in% c(1, 2), ]
# Xvalid.tmp = X[id_tv == 3, ]
# ytrain.tmp = y[id_tv %in% c(1, 2)]
# yvalid.tmp = y[id_tv == 3]
# # fit
btree = getSupervisedTree(X, y, linkage, allow.noise)
# Xtrainb = computeBalances(btree, Xtrain.tmp)
# Xvalidb = computeBalances(btree, Xvalid.tmp)
Xb = computeBalances(btree, X)
# lasso fit
glm.temp = fitGlmnet(Xb, y, lambda)
return(list(
btree = btree,
betahat = as.vector(glm.temp$beta),
lambda = glm.temp$lamdba
))
}
fitCOATLasso = function(X, y, linkage, lambda = NULL){
# get COAT tree
d_coat = 1 - coat(X)$corr
rownames(d_coat) = colnames(X)
colnames(d_coat) = colnames(X)
coat_tree = hclust(as.dist(d_coat),method = linkage)
# compute balances
sbp_coat = sbp.fromHclust(coat_tree)
Xb = balance.fromSBP(X, sbp_coat)
# lasso fit
glm.temp = fitGlmnet(Xb, y, lambda)
return(list(
glmnet = glm.temp$glmnet,
btree = coat_tree,
betahat = as.vector(glm.temp$beta),
lambda = glm.temp$lamdba
))
}
test_slrLASSO = fitSLRLasso(X.prop.tr, y.tr, linkage = "complete") # works now!
# fit to test data
slrLassofit = function(x){
# names(x) = test_slrLASSO$btree$labels
xb = computeBalances(test_slrLASSO$btree, x)
predict(test_slrLASSO$glm, newx = xb, type = "response")
}
slrLassoyhat = slrLassofit(X.prop.te)
slrLassoMSE = mean(slrLassoyhat - y.te)^2
# plot dendrogram
slr_btree = test_slrLASSO$btree
slr_btree$labels = 1:length(slr_btree$labels)
plot(slr_btree)
### trying coat
test_coatLASSO = fitCOATLasso(X.prop.tr, y.tr, linkage = "complete")
coatLassofit = function(x){
# names(x) = test_slrLASSO$btree$labels
xb = computeBalances(test_coatLASSO$btree, x)
xb %*% test_coatLASSO$betahat
}
predict(test_slrLASSO$glmnet, newx = xb, type = "response")
# fit to test data
slrLassofit = function(x){
# names(x) = test_slrLASSO$btree$labels
xb = computeBalances(test_slrLASSO$btree, x)
predict(test_slrLASSO$glmnet, newx = xb, type = "response")
}
slrLassoyhat = slrLassofit(X.prop.te)
slrLassoMSE = mean(slrLassoyhat - y.te)^2
# plot dendrogram
slr_btree = test_slrLASSO$btree
slr_btree$labels = 1:length(slr_btree$labels)
plot(slr_btree)
### trying coat
test_coatLASSO = fitCOATLasso(X.prop.tr, y.tr, linkage = "complete")
coatLassofit = function(x){
# names(x) = test_slrLASSO$btree$labels
xb = computeBalances(test_coatLASSO$btree, x)
predict(test_coatLASSO$glmnet, newx = xb, type = "response")
# xb %*% test_coatLASSO$betahat
}
coatLassoyhat = coatLassofit(X.prop.te)
coatLassoMSE = mean(coatLassoyhat - y.te)^2
# plot dendrogram
coat_btree = test_coatLASSO$btree
coat_btree$labels = 1:length(coat_btree$labels)
plot(coat_btree)
# trying principle balances
test_pbLASSO = fitPBLasso(X.prop.tr, y.tr, lambda = NULL)
# seeing proportion of 0 beta elts
comp.0beta = sum(betahatcompositional == 0) / length(betahatcompositional)
classic.0beta = sum(betahatclassic == 0) / length(betahatclassic)
slr.0beta = sum(test_slrLASSO$betahat == 0) / length(test_slrLASSO$betahat)
coat.0beta = sum(test_coatLASSO$betahat == 0) / length(test_coatLASSO$betahat)
compare.0beta = data.frame(compositionalLasso = comp.0beta,
classicLasso = classic.0beta,
coatLasso = coat.0beta,
slrLasso = slr.0beta)
compare.0beta
# compare MSEs of three methods
compare.mse = data.frame(compositionalLasso = compositionalLassoMSE,
classicLasso = classicLassoMSE,
coatLasso = coatLassoMSE,
slrLasso = slrLassoMSE)
compare.mse
