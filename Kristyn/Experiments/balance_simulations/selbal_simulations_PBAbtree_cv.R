# last updated: 04/06/2021
# selbal simulations from response generated by PBA balances

getwd()
output_dir = "Kristyn/Experiments/balance_simulations/output"

# libraries
library(mvtnorm)
library(stats) # for hclust()
library(balance) # for sbp.fromHclust()

# set up parallelization
library(foreach)
library(future)
library(doFuture)
library(parallel)
registerDoFuture()
nworkers = detectCores()
plan(multisession, workers = nworkers)

library(rngtools)
library(doRNG)
rng.seed = 123 # 123, 345
registerDoRNG(rng.seed)

# Dr. Ma sources
library(Matrix)
library(glmnet)
library(compositions)
library(stats)
source("RCode/func_libs.R")

# Kristyn sources
functions_path = "Kristyn/Functions/"
source(paste0(functions_path, "supervisedlogratios.R"))

# Other methods
library(selbal)

# Method Settings
opt.cri = "max"
K = 5
intercept = TRUE
# n.iter? default is 10

# Simulation settings
numSims = 100
n = 100
p = 200
rho = 0.2 # 0.2, 0.5
# indices.theta = sample(1:(p - 1), 5, replace = FALSE) # choose bt 1 and p - 1
# indices.theta = c(159, 179, 14, 195, 170) # the randomly chosen, above
# indices.theta = 1 # saturated -- all 200 taxa
indices.theta = p - 1 # sparse -- only 2 taxa
# indices.theta = c(197, 198, 199) # log ratios with 2 taxa each
values.theta = NULL
sigma_eps = 0.5
seed = 1

muW = c(
  rep(log(p), 5), 
  rep(0, p - 5)
)
SigmaW = matrix(0, p, p)
for(i in 1:p){
  for(j in 1:p){
    SigmaW[i, j] = rho^abs(i - j)
  }
}

################################################################################
# Simulations #
################################################################################

data.sims = foreach(
  b = 1:numSims
) %dorng% {
  library(limSolve)
  library(mvtnorm)
  library(Matrix)
  library(glmnet)
  library(compositions)
  library(stats)
  library(balance) # for sbp.fromHclust()
  
  source("RCode/func_libs.R")
  source(paste0(functions_path, "supervisedlogratios.R"))
  
  nlam = 200
  
  # simulate training data #
  # generate W
  W = rmvnorm(n = n, mean = muW, sigma = SigmaW) # n x p
  # let X = exp(w_ij) / (sum_k=1:p w_ik) ~ Logistic Normal (the covariates)
  V = exp(W)
  rowsumsV = apply(V, 1, sum)
  X = V / rowsumsV
  sbp = sbp.fromPBA(X) # contrasts matrix, a.k.a. sbp matrix
  U = getU(sbp = sbp) # U
  epsilon = rnorm(n, 0, sigma_eps)
  Xb = computeBalances(X, U = U) # ilr(X) # ilr(X)
  # get theta
  theta = rep(0, p - 1)
  if(is.null(values.theta)){
    theta[indices.theta] = 1
  } else{
    if(length(indices.theta) == length(values.theta)){
      stop("indices.theta does not have same length as values.theta")
    }
    theta[indices.theta] = values.theta
  }
  theta = as.matrix(theta)
  # get beta
  beta = getBeta(theta, U = U)
  # generate Y
  Y = Xb %*% theta + epsilon
  
  # simulate test data #
  # simulate independent test set of size n
  # generate W
  W.test = rmvnorm(n = n, mean = muW, sigma = SigmaW) # n x p
  # let X = exp(w_ij) / (sum_k=1:p w_ik) ~ Logistic Normal (the covariates)
  V.test = exp(W.test)
  rowsumsV.test = apply(V.test, 1, sum)
  X.test = V.test / rowsumsV.test
  sbp.test = sbp.fromPBA(X.test) # contrasts matrix, a.k.a. sbp matrix
  U.test = getU(sbp = sbp.test) # U
  epsilon.test = rnorm(n, 0, sigma_eps)
  Xb.test = computeBalances(X.test, U = U.test) # ilr(X)
  # generate Y
  Y.test = Xb.test %*% theta + epsilon.test
  
  list(X = X, 
       Xb = Xb, 
       Y = Y, 
       U = U, 
       theta = theta, 
       beta = beta, 
       X.test = X.test, 
       Xb.test = Xb.test, 
       Y.test = Y.test, 
       U.test = U.test)
}

# set.seed(1)
evals = matrix(NA, nrow = 9, ncol = numSims)
for(b in 1:numSims){
  library(limSolve)
  library(mvtnorm)
  library(Matrix)
  library(glmnet)
  library(compositions)
  library(stats)
  library(balance) # for sbp.fromHclust()
  library(selbal)
  
  source("RCode/func_libs.R")
  source(paste0(functions_path, "supervisedlogratios.R"))
  
  # training data #
  X = data.sims[[b]]$X
  U = data.sims[[b]]$U
  Xb = data.sims[[b]]$Xb
  theta = data.sims[[b]]$theta
  beta = data.sims[[b]]$beta
  Y = data.sims[[b]]$Y
  
  # test data #
  X.test = data.sims[[b]]$X.test
  U.test = data.sims[[b]]$U.test
  Xb.test = data.sims[[b]]$Xb.test
  Y.test = data.sims[[b]]$Y.test
  
  # apply supervised log-ratios, using CV to select lambda=
  rownames(X) = paste("Sample", 1:nrow(X), sep = "_")
  colnames(X) = paste("V", 1:ncol(X), sep = "")
  selbal.fit = selbal.cv(x = X, y = as.vector(Y), n.fold = K)
  
  # PBA selbal
  pba.selbal = rep(0, p)
  names(pba.selbal) = colnames(X)
  pba.pos = unlist(subset(selbal.fit$global.balance,
                          subset = Group == "NUM", select = Taxa))
  r = length(pba.pos)
  pba.neg = unlist(subset(selbal.fit$global.balance, 
                          subset = Group == "DEN", select = Taxa))
  s = length(pba.neg)
  pba.selbal[pba.pos] = 1 / r
  pba.selbal[pba.neg] = -1 / s
  
  # U selbal
  norm.const = sqrt((r * s) / (r + s))
  u.selbal = norm.const * pba.selbal
  
  # check: these are equal
  # lm(as.vector(Y) ~ log(X) %*% as.matrix(u.selbal))
  # selbal.fit$glm
  
  thetahat = coefficients(selbal.fit$glm)[2]
  betahat = u.selbal %*% as.matrix(thetahat)
  
  # evaluate model #
  # 1. prediction error #
  # 1a. on training set #
  # get prediction error on training set
  # Yhat.train = a0 + computeBalances(X, btree) %*% thetahat
  Yhat.train = predict.glm(selbal.fit$glm, 
                           newdata = data.frame(X), 
                           type = "response")
  PE.train = crossprod(Y - Yhat.train) / n
  # 1b. on test set #
  # get prediction error on test set
  # Yhat.test = a0 + computeBalances(X.test, btree) %*% thetahat
  rownames(X.test) = paste("Sample", 1:nrow(X), sep = "_")
  colnames(X.test) = paste("V", 1:ncol(X), sep = "")
  Yhat.test = predict.glm(selbal.fit$glm, newdata = data.frame(X.test), 
                           type = "response")
  PE.test = crossprod(Y.test - Yhat.test) / n
  # 2. estimation accuracy #
  # 2a. estimation of beta #
  betahat = as.matrix(u.selbal) %*% as.matrix(coefficients(selbal.fit$glm)[2])
  EA1 = sum(abs(betahat - beta))
  EA2 = sqrt(crossprod(betahat - beta))
  EAInfty = max(abs(betahat - beta))
  # 3. selection accuracy #
  # 3a. selection of beta #
  non0.beta = (beta != 0)
  non0s = sum(non0.beta)
  non0.betahat = (betahat != 0)
  # FP
  FP = sum((non0.beta != non0.betahat) & non0.betahat)
  # FN
  FN = sum((non0.beta != non0.betahat) & non0.beta)
  # TPR
  TPR = sum((non0.beta == non0.betahat) & non0.betahat) / sum(non0.beta)
  # return
  evals[, b] = c(PE.train, PE.test, EA1, EA2, EAInfty, FP, FN, TPR, 
                 sum(non0.beta))
}
rownames(evals) = c("PEtr", "PEte", "EA1", "EA2", "EAInfty", "FP", "FN", "TPR", 
                    "betaSparsity")

eval.means = apply(evals, 1, mean)
eval.sds = apply(evals, 1, sd)
eval.ses = eval.sds / sqrt(numSims)
evals.df = data.frame("mean" = eval.means, "sd" = eval.sds, "se" = eval.ses)
evals.df

saveRDS(
  evals, 
  file = paste0(output_dir,
                "/selbal_cv_sims", 
                "_PBA", 
                "_theta_", paste(indices.theta, collapse = "_"),
                "_dim", n, "x", p, 
                "_rho", rho, 
                "_int", intercept,
                "_seed", rng.seed,
                ".rds"))
saveRDS(
  evals.df, 
  file = paste0(output_dir,
                "/selbal_cv_summaries", 
                 "_PBA", 
                "_theta_", paste(indices.theta, collapse = "_"),
                "_dim", n, "x", p, 
                "_rho", rho, 
                "_int", intercept,
                "_seed", rng.seed,
                ".rds"))
