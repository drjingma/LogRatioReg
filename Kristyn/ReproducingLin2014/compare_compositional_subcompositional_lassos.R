workdir = "/home/kristyn/Documents/research/supervisedlogratios/LogRatioReg"
setwd(workdir)

# libraries
library(mvtnorm)
library(balance)
library(selbal)
library(propr)
library(microbenchmark)
library(ggplot2)
library(logratiolasso)
image_path = "/home/kristyn/Pictures"

# Dr. Ma sources
source("RCode/func_libs.R")
source("COAT-master/coat.R")

# Kristyn sources
functions_path = "Kristyn/Functions/"
source(paste0(functions_path, "classic_lasso.R"))
source(paste0(functions_path, "compositional_lasso.R"))
source(paste0(functions_path, "supervisedlogratios.R"))
source(paste0(functions_path, "coat.R"))
source(paste0(functions_path, "principlebalances.R"))
source(paste0(functions_path, "propr.R"))
source(paste0(functions_path, "selbal.R"))


################################################################################
# 1-fold CV results comparison #
################################################################################

# data
DataFolder <- "/Data/"
load(paste0(workdir, DataFolder, "BMI.rda"))
dim(raw_data) # 98 x 89
dim(X) # 98 x 87
dim(X.prop) # 98 x 87

# preprocessing: take out rare taxa/OTU, to avoid 0-vector columns when splitting
#   into training and test sets
prop_zero_per_sample = as.vector(apply(X, 1, function(a) sum(a == 0.5) / ncol(X)))
prop_zero_per_otu = as.vector(apply(X, 2, function(a) sum(a == 0.5) / nrow(X)))
X = X[, prop_zero_per_otu < 0.95] # take out OTUs with less than 5% nonzero
X.prop = X.prop[, prop_zero_per_otu < 0.97]

# separate into training and test sets
# Split the data into K folds
n = dim(X)[1]
set.seed(2)
shuffle = sample(1:n)
id_traintest = (shuffle %% 2) + 1
n_traintest = as.vector(table(id_traintest))
X.tr = X[id_traintest == 1, ]
X.prop.tr = sweep(X.tr, MARGIN = 1, STATS = rowSums(X.tr), FUN = "/")
X.te = X[id_traintest == 2, ]
X.prop.te = sweep(X.te, MARGIN = 1, STATS = rowSums(X.te), FUN = "/")
y.tr = y[id_traintest == 1]
y.te = y[id_traintest == 2]

# shared settings (for subcompositional and compositional lasso)
cv.folds = 10
tol = 1e-4
seed = 0
n_lambda = 50
# here, I use the same lambda's in both (generated by the subcompositional one)

################################################################################
#  subcompositional lasso
################################################################################

# y = log(X) beta + epsilon
# subject to constraint sum_{i = 1}^p beta_i == 1

# betahat from subcompositional Lasso
test_cvSubCompLASSO =  cv.func(
  method="ConstrLasso", y.tr, log(X.prop.tr), 
  Cmat = matrix(1, dim(X.prop.tr)[2], 1), lambda = NULL, nlam = n_lambda, 
  intercept = TRUE, scaling = TRUE, nfolds = cv.folds, maxiter = 10e5, 
  tol = tol, seed=seed)

betahatsubcomp = test_cvSubCompLASSO$bet[, which.min(test_cvSubCompLASSO$cvm)]
betahat0subcomp = test_cvSubCompLASSO$int[which.min(test_cvSubCompLASSO$cvm)]

# check constraint
sum(betahatsubcomp) # very small

# get predicting function
subcompLassofit = function(x){
  betahat0subcomp + log(x) %*% betahatsubcomp
}

# evaluate
subcompLassoyhat = apply(X.prop.te, 1, subcompLassofit)
subcompLassoMSE = mean(subcompLassoyhat - y.te)^2

# without scaling
test_cvSubCompLASSO2 =  cv.func(
  method="ConstrLasso", y.tr, log(X.prop.tr), 
  Cmat = matrix(1, dim(X.prop.tr)[2], 1), lambda = NULL, nlam = n_lambda, 
  intercept = TRUE, scaling = FALSE, nfolds = cv.folds, maxiter = 10e5, 
  tol = tol, seed=seed)
cvm_idx = which.min(test_cvSubCompLASSO2$cvm)
betahatsubcomp2 = test_cvSubCompLASSO2$bet[, cvm_idx]
betahat0subcomp2 = test_cvSubCompLASSO2$int[cvm_idx]
# check constraint
sum(betahatsubcomp2) # very small
# get predicting function
subcompLassofit2 = function(x){
  betahat0subcomp2 + log(x) %*% betahatsubcomp2
}
# evaluate
subcompLassoyhat2 = apply(X.prop.te, 1, subcompLassofit2)
subcompLassoMSE2 = mean(subcompLassoyhat2 - y.te)^2

################################################################################
#  compositional lasso - matching standardization method
################################################################################
source(paste0(functions_path, "compositional_lasso.R"))
# y = log(X) beta + epsilon
# subject to constraint sum_{i = 1}^p beta_i == 0

# betahat from compositional Lasso
set.seed(seed)
test_cvCompLASSO = cvCompositionalLASSO.match(
  log(X.prop.tr), y.tr, 
  # lambda_seq = test_cvSubCompLASSO$lambda, 
  n_lambda = n_lambda, k = cv.folds, intercept = TRUE, scaling = TRUE)
betahatcomp = test_cvCompLASSO$beta_mat[, test_cvCompLASSO$cvm_idx]
betahat0comp = test_cvCompLASSO$beta0_vec[test_cvCompLASSO$cvm_idx]

# check constraint
sum(betahatcomp)

# get predicting function
compLassofit = function(x){
  betahat0comp + log(x) %*% betahatcomp
}

# evaluate
compLassoyhat = apply(X.prop.te, 1, compLassofit)
compLassoMSE = mean(compLassoyhat - y.te)^2

# without scaling
set.seed(seed)
test_cvCompLASSO2 = cvCompositionalLASSO.match(
  log(X.prop.tr) ,y.tr, 
  # lambda_seq = test_cvSubCompLASSO2$lambda, 
  n_lambda = n_lambda, k = cv.folds, intercept = TRUE, scaling = FALSE)
betahatcomp2 = test_cvCompLASSO2$beta_mat[, test_cvCompLASSO2$cvm_idx]
betahat0comp2 = test_cvCompLASSO2$beta0_vec[test_cvCompLASSO2$cvm_idx]
# check constraint
sum(betahatcomp2)
# get predicting funciton
compLassofit2 = function(x){
  betahat0comp2 + log(x) %*% betahatcomp2
}
# evaluate
compLassoyhat2 = apply(X.prop.te, 1, compLassofit2)
compLassoMSE2 = mean(compLassoyhat2 - y.te)^2

################################################################################
#  compositional lasso - NOT matching
################################################################################
source(paste0(functions_path, "compositional_lasso.R"))
# y = log(X) beta + epsilon
# subject to constraint sum_{i = 1}^p beta_i == 0

# betahat from compositional Lasso
set.seed(seed)
test_cvCompLASSOv1 = cvCompositionalLASSO(
  log(X.prop.tr), y.tr, 
  # lambda_seq = test_cvSubCompLASSO$lambda, 
  n_lambda = n_lambda, k = cv.folds, eps = 1e-2)
betahatcompv1 = test_cvCompLASSOv1$beta_mat[, test_cvCompLASSOv1$cvm_idx]
betahat0compv1 = test_cvCompLASSOv1$beta0_vec[test_cvCompLASSOv1$cvm_idx]

# check constraint
sum(betahatcompv1)

# get predicting function
compLassofitv1 = function(x){
  betahat0compv1 + log(x) %*% betahatcompv1
}

# evaluate
compLassoyhatv1 = apply(X.prop.te, 1, compLassofitv1)
compLassoMSEv1 = mean(compLassoyhatv1 - y.te)^2

################################################################################
#  compositional vs subcompositional lasso
################################################################################

# compare to my compositional lasso (scaled, too)
print(cbind(betahatsubcomp, betahatcomp, betahatcompv1))
print(c(betahat0subcomp, betahat0comp, betahat0compv1))
sum((betahatsubcomp - betahatcomp)^2)
which(betahatsubcomp != 0)
which(betahatcomp != 0)

# compare to my compositional lasso (not scaled, too)
print(cbind(betahatsubcomp2, betahatcomp2))
sum((betahatsubcomp2 - betahatcomp2)^2)
which(betahatsubcomp2 != 0)
which(betahatcomp2 != 0)

















################################################################################
################################################################################
################################################################################
################################################################################
################################################################################

################################################################################
# Lin et al attempt to reproduce 10-fold CV and bootstrap #
################################################################################

# settings

# Cross-validation
cv.seed = 1234
cv.n_lambda = 100

# Bootstrap
bs.seed = 1
bs.n = 100

# data
# 98 samples, 87 genera
# replace zero counts with 0.5 (maximum rounding error)
DataFolder <- "/Data/"
load(paste0(workdir, DataFolder, "BMI.rda"))
# dim(raw_data) # 98 x 89
# dim(X) # 98 x 87
# dim(X.prop) # 98 x 87
n = dim(X)[1]
num.genera = dim(X)[2]

################################################################################
# Choosing the tuning parameter -- an example of what goes on in the bootstrap #
################################################################################

# Lin et. al. 2014 applied the proposed method using a refitted version of 
#   ten-fold cross-validation to choose the tuning parameter, where
#   the prediction of each sample split is computed with the refitted coeffs
#   (obtained after model selection and without penalization)

# Split the data into 10 folds
cv.K = 10
set.seed(cv.seed)
shuffle = sample(1:n)
idfold = (shuffle %% cv.K) + 1
n_fold = as.vector(table(idfold))

################################################################################
# Lin et al. 2014 : Compositional Lasso
################################################################################

# Do cross-validation
# calculate squared error (prediction error?) for each fold, 
#   needed for CV(lambda) calculation
cvm = rep(NA, cv.n_lambda) # want to have CV(lambda)
cvm_sqerror = matrix(NA, cv.K, cv.n_lambda)
# Fit Lasso for each fold removed
for (j in 1:cv.K){
  # Training data
  Xtrain = X.prop[idfold != j, ]
  Ytrain = y[idfold != j]
  # Test data
  Xtest = X.prop[idfold == j, ]
  Ytest = y[idfold == j]
  
  # Fit LASSO on that fold using fitLASSOcompositional
  # first, take out columns that have all 0.5's, because they shouldn't be selected anyway (and lead to problems)
  cols.0.5 = apply(Xtrain, 2, FUN = function(vec) all(vec == 0.5))
  Xtrain = Xtrain[, !cols.0.5]
  Xtest = Xtest[, !cols.0.5]
  XYdata = data.frame(Xtrain, y = Ytrain)
  Lasso_j = fitCompositionalLASSO(Xtrain ,Ytrain, n_lambda = cv.n_lambda) # a problem in centering and scaling X cols with all 0.5's
  non0.betas = Lasso_j$beta_mat != 0 # diff lambda = diff col
  for(m in 1:cv.n_lambda){
    # get refitted coefficients, after model selection and w/o penalization
    selected_variables = non0.betas[, m]
    if(all(!selected_variables)){ # if none selected
      refit = lm(y ~ 1, data = XYdata)
    } else{ # otherwise, fit on selected variables
      refit = lm(
        as.formula(paste("y", "~",
                         paste(colnames(XYdata)[which(selected_variables)], 
                               collapse = "+"),
                         sep = "")),
        data=XYdata)
    }
    # calculate squared error (prediction error?)
    # if(!all.equal(colnames(Xtest), colnames(Xtrain))) stop("Xtrain and Xtest don't have the same variable names")
    newx = Xtest[, selected_variables, drop = FALSE]
    Ypred = predict(refit, newdata = data.frame(newx), type = "response")
    cvm_sqerror[j, m] = sum(crossprod(Ytest - Ypred))
  }
}

# Calculate CV(lambda) for each value of lambda
cvm = (1 / n) * colSums(cvm_sqerror)

# Find lambda_min = argmin{CV(lambda)}
lambda_min_index = which.min(cvm)
lambda_min = Lasso_j$lambda_seq[lambda_min_index] # all Lasso_j generate the same lambda_seq
lambda_seq2 = Lasso_j$lambda_seq

# final fit
Lasso_select = fitCompositionalLASSO(X.prop, y, lambda_min)
XYdata = data.frame(X.prop, y = y)
non0.betas = Lasso_select$beta_mat != 0 # diff lambda = diff col
selected_variables = non0.betas
if(all(!selected_variables)){ # if none selected
  finalfit = lm(y ~ 1, data = XYdata)
} else{ # otherwise, fit on selected variables
  finalfit = lm(
    as.formula(paste("y", "~",
                     paste(colnames(XYdata)[which(selected_variables)], 
                           collapse = "+"),
                     sep = "")),
    data=XYdata)
}
finalfit


################################################################################
# Shi et al. 2016 : Sub-compositional Lasso
################################################################################

# Do cross-validation
# calculate squared error (prediction error?) for each fold, 
#   needed for CV(lambda) calculation
cvm2 = rep(NA, cv.n_lambda) # want to have CV(lambda)
cvm_sqerror2 = matrix(NA, cv.K, cv.n_lambda)
# Fit Lasso for each fold removed
for (j in 1:cv.K){
  # Training data
  Xtrain = X.prop[idfold != j, ]
  Ytrain = y[idfold != j]
  # Test data
  Xtest = X.prop[idfold == j, ]
  Ytest = y[idfold == j]
  
  # Fit LASSO on that fold using fitLASSOcompositional
  # first, take out columns that have all 0.5's, because they shouldn't be selected anyway (and lead to problems)
  cols.0.5 = apply(Xtrain, 2, FUN = function(vec) all(vec == 0.5))
  Xtrain = Xtrain[, !cols.0.5]
  Xtest = Xtest[, !cols.0.5]
  XYdata = data.frame(Xtrain, y = Ytrain)
  ##############################################################################
  Lasso_j = ConstrLasso(
    Ytrain, Xtrain, Cmat = rep(1, num.genera), lambda=NULL, nlam=20, intercept=TRUE, scaling=TRUE, maxiter=1000, tol=1e-8)
  # Lasso_j = fitCompositionalLASSO(Xtrain ,Ytrain, n_lambda = cv.n_lambda) # a problem in centering and scaling X cols with all 0.5's
  ##############################################################################
  non0.betas = Lasso_j$beta_mat != 0 # diff lambda = diff col
  for(m in 1:cv.n_lambda){
    # get refitted coefficients, after model selection and w/o penalization
    selected_variables = non0.betas[, m]
    if(all(!selected_variables)){ # if none selected
      refit = lm(y ~ 1, data = XYdata)
    } else{ # otherwise, fit on selected variables
      refit = lm(
        as.formula(paste("y", "~",
                         paste(colnames(XYdata)[which(selected_variables)], 
                               collapse = "+"),
                         sep = "")),
        data=XYdata)
    }
    # calculate squared error (prediction error?)
    # if(!all.equal(colnames(Xtest), colnames(Xtrain))) stop("Xtrain and Xtest don't have the same variable names")
    newx = Xtest[, selected_variables, drop = FALSE]
    Ypred = predict(refit, newdata = data.frame(newx), type = "response")
    cvm_sqerror2[j, m] = sum(crossprod(Ytest - Ypred))
  }
}

# Calculate CV(lambda) for each value of lambda
cvm2 = (1 / n) * colSums(cvm_sqerror2)

# Find lambda_min = argmin{CV(lambda)}
lambda_min_index2 = which.min(cvm2)
lambda_min2 = Lasso_j$lambda_seq[lambda_min_index2]

# final fit
################################################################################
Lasso_select2 #= fitCompositionalLASSO(X.prop, y, lambda_min)
################################################################################
XYdata = data.frame(X.prop, y = y)
non0.betas2 = Lasso_select2$beta_mat != 0 # diff lambda = diff col
selected_variables2 = non0.betas2
if(all(!selected_variables2)){ # if none selected
  finalfit2 = lm(y ~ 1, data = XYdata)
} else{ # otherwise, fit on selected variables
  finalfit2 = lm(
    as.formula(paste("y", "~",
                     paste(colnames(XYdata)[which(selected_variables2)], 
                           collapse = "+"),
                     sep = "")),
    data=XYdata)
}
finalfit2