=---
title: "R Notebook"
output:
  pdf_document: default
  html_notebook: default
---

```{r}
library(dplyr)
library(glmnet)
library(ggplot2)
library(plotROC)
```

```{r}
load("xy.Rdata")
load("xyval.Rdata")
```

We add 1 to each entry of the covariate matrix to ensure the entries are positive.

```{r}
del <- 1
z <- log(x + del)
zval <- log(xval + del)
```

```{r}
#reindex the response to be 0-1
y <- y - 1
yval <- yval - 1
```

We have to do blockwise CV because observations belonging to the same patient are dependent.

```{r}
set.seed(1)
permed <- sample(1:36, 36, replace = FALSE)
fold_id <- permed[patid] %/% 3 + 1 
```

## Lasso model
(Good CV)

We first fit a lasso on the unpaired logs.

```{r}
set.seed(1)
#random partition of individuals
lasso_fit <- cv.glmnet(z, y, family = "binomial", foldid = fold_id)
lasso_fit2 <- cv.glmnet(z, y, foldid = fold_id)


sum(coef(lasso_fit, s ="lambda.min")[-1] != 0)
cv_l1 <- sum(abs(coef(lasso_fit, s ="lambda.min")[-1]))
coef(lasso_fit, s ="lambda.min")
coef(lasso_fit, s ="lambda.1se")

#save these for future plotting
v26 <- coef(lasso_fit, s ="lambda.min")[27]
v19 <- coef(lasso_fit, s ="lambda.min")[20]
```

Make the path plot for the presentation.

```{r}
pdf("plots/unconstrained-path.pdf")
par(cex = 1.3)
lasso_fit3 <- glmnet(z, y, family = "binomial", pmax = 19) #make plot with small number of variables
plot(lasso_fit3)
abline(v = cv_l1, lty = 2)
points(x = c(cv_l1, cv_l1), y = c(v26, v19), type = "p", lwd = 15) 
dev.off()
```

## Constrained lasso model

```{r}
source("logRatioLasso.R")
```

```{r}
set.seed(1)
constr_lasso_fit <- glmnet.constr(z, y, family = "binomial")
constr_lasso_fit_cv <- cv.glmnet.constr(constr_lasso_fit, z, y, foldid = fold_id)
best <- which.min(constr_lasso_fit_cv$cvm)
beta_lrl <- constr_lasso_fit$beta[, best]
sum(beta_lrl != 0)
beta_lrl

#extract some values for plotting on lasso path
cv_l1_lrl <- sum(abs(beta_lrl))
v19_lrl <- abs(beta_lrl[19])
v26_lrl <- abs(beta_lrl[26])
```

We can plot the lasso path:

```{r}
#handle the sign flips
constr_lasso_fit$glmnet.obj$beta <- constr_lasso_fit$beta


pdf("plots/constrained-path.pdf")
par(cex = 1.3)
plot(constr_lasso_fit$glmnet.obj)
abline(v = cv_l1_lrl, lty = 2)
points(x = c(cv_l1_lrl, cv_l1_lrl), 
       y = c(v19_lrl, -v26_lrl), lwd = 15)
dev.off()
```

```{r}
#pdf("plots/combined-path.pdf", width = 7, height = 3.5)

setEPS()
postscript("plots/combined-path.eps", width = 5.5, height = 3)
par(mfrow = c(1,2), cex = 1)

lasso_fit3 <- glmnet(z, y, family = "binomial", pmax = 19) #make plot with small number of variables
plot(lasso_fit3)
abline(v = cv_l1, lty = 2)
points(x = c(cv_l1, cv_l1), y = c(v26, v19), type = "p", lwd = 5) 

plot(constr_lasso_fit$glmnet.obj)
abline(v = cv_l1_lrl, lty = 2)
points(x = c(cv_l1_lrl, cv_l1_lrl), 
       y = c(v19_lrl, -v26_lrl), lwd = 5)

dev.off()
```


## Single ratio model

```{r}
z_m <- z[,19] - z[,26]
data <- data.frame(y,z_m)
data <- data.frame(y,z_m)
simple_fit <- glm(y ~ z_m, data = data, family = binomial)
summary(simple_fit)
```



## Prediction error

#### Oracle model

```{r}
z_m_val <- zval[,19] - zval[,26]
pred_data <- data.frame(yval, z_m = z_m_val)
simple_pred <- predict(simple_fit, newdata = pred_data, type = "response")
sum((simple_pred > .5) == (yval == 1)) / 202
```

#### Vanilla Lasso

```{r}
lasso_pred <- predict(lasso_fit, zval, type = "response", s = "lambda.min")
lasso_pred2 <- predict(lasso_fit2, zval, type = "response", s = "lambda.min")
sum(lasso_pred > .5) / 202
sum((lasso_pred > .5) == (yval == 1)) / 202

# calibrate at mean
yhat = lasso_pred > mean(lasso_pred)
mean(yhat == yval)
noise_estimate <- mean((yhat - yval)^2)

#alternative: calibrate to know ratio on test data:
sum(yval == 1)
sum(lasso_pred > .91)  #notice that the model is very poorly calibrated for the test data
sum((lasso_pred > .91) == (yval == 1)) / 202 #better accuracy
```


#### Ridge

```{r}
ridge_fit <- cv.glmnet(z, y, family = "binomial", foldid = fold_id, alpha = 0)
ridge_pred <- predict(ridge_fit, zval, type = "response", s = "lambda.min")
sum(ridge_pred > .5) / 202
sum((ridge_pred > .5) == (yval == 1)) / 202

# calibrate at mean
yhat = ridge_pred > mean(ridge_pred)
mean(yhat == yval)
```


#### Constrained lasso

```{r}
lrl_fit <- z  %*% beta_lrl
lrl_pred <- zval %*% beta_lrl

sum(y == 1)
intercept <- -4.5
sum(exp(lrl_fit + intercept) / 1 + exp(lrl_fit + intercept))
lrl_pred_response <- exp(lrl_pred + intercept) / 1 + exp(lrl_pred + intercept) #account for intercept

sum(lrl_pred_response > .5) / 202 #check predicted ratio
sum((lrl_pred_response > .5) == (yval == 1)) / 202

#calibrate at mean
yhat = lrl_pred > mean(lrl_pred)
mean(yhat == yval)

#alternative: match to test data
sum(yval == 1)
sum(lrl_pred > 2.32)  #calibrate procedure to predict at the same ratio as the validation population (since this model doesn't come with an intercept)
sum((lrl_pred > 2.32) == (yval == 1)) / 202
```

The LRL has slightly better predictive performance than the vanilla lasso and recover the "correct" ratio.  Notice this has worse performance than the ``oracle model", which has no noise variables.

## approximate forward stepwise

Here we have to treat the response as gaussian.

```{r}
source("approximate_fs.r")
```

```{r}
approx_fs_model <- approximate_fs(z, y, k_max = 25)
approx_fs_cvm <- cv.approximate_fs(z, y, fold_id = fold_id, k_max = 25)
approx_fs_cvm$cvm
which.min(approx_fs_cvm$cvm)
```

This method selects 6 log-ratios.

```{r}
approx_fs_model$ratios
```

feature 26 enters right away, but 19 is not selected until step 13.

```{r}
fs_pred <- predict.approximate_fs(approx_fs_model, zval)[, 6]
fs_pred2 <- fs_pred > mean(fs_pred)
mean(fs_pred2 == yval)
```

We have about 72% prediction accuracy.

## two-step

```{r}
library(glmnet)
library(class)
library("sparsenet")
source("rob/pancreas.morefuns.R")
source("rob/thyroid.morefuns.R")
source("rob/zina.myfuns.R")
source("rob/myfuns.R")
source("rob/logRatioLasso.R")
```

```{r}
load("xy.RData")
load("xyval.RData")
y=y-1
yval=yval-1


a=logRatioLasso(x,y)
aa=cv.logRatioLasso(a,x,y,keep=T)
aaa=predict.logRatioLasso(a,xval,ilist=aa$ijmin[1],jlist=aa$ijmin[2])
ts_pred <- aaa$yhat

noise_estimate_ts <- mean((aaa$yhat - yval)^2)

mean((aaa$yhat > mean(aaa$yhat)) == yval)
mean((aaa$yhat > median(aaa$yhat)) == yval)
table(yval, (aaa$yhat > mean(aaa$yhat)))
```

## Boxplots

```{r}
plot_data <- data.frame("lasso_pred" = lasso_pred, 
                          ts_pred = aaa$yhat, truth = as.factor(yval))
ts_boxplot <- ggplot(plot_data, aes(x = truth, y = ts_pred)) +
  geom_boxplot(color = "blue") +
  scale_y_continuous(limits=c(-.25,1)) +
  labs(y = "Two-stage prediction", x = "Cancer status")

lasso_boxplot <- ggplot(plot_data, aes(x = truth, y = lasso_pred)) +
  scale_y_continuous(limits=c(-.25,1)) +
  geom_boxplot() +
  labs(y ="Lasso logistic regression prediction", x = "Cancer status")

dual_boxplot <- grid.arrange(ts_boxplot, lasso_boxplot, ncol = 2)
```

```{r}
#ggsave("plots/proteomics-analysis/prediction-boxplots.pdf", dual_boxplot, device = "pdf")
```


## ROC plots

```{r}
#ts_pred_obj <- prediction(ts_pred, yval)
#lasso_pred_obj <- prediction(lasso_pred, yval)

#ts_perf <- performance(ts_pred_obj, measure = "tpr", x.measure = "fpr")
#lasso_perf <- performance(lasso_pred_obj, measure = "tpr", x.measure = "fpr")

# plot(ts_perf)
# plot(lasso_perf)

ts_roc <- ggplot(plot_data, aes(d = as.numeric(truth), m = ts_pred)) +
  geom_roc(aes(d = as.numeric(truth), m = lasso_pred), 
           color = "black",
           labels = FALSE,
           n.cuts = 0) + 
  geom_roc(color = "blue", labels = FALSE, n.cuts = 0) +
  labs(y = "True positive fraction", x = "False positive fraction")
ts_roc
```

AUC calculation:
```{r}
ts_auc <- performance(ts_pred_obj, "auc")@y.values[[1]]
lasso_auc <- performance(lasso_pred_obj, "auc")@y.values[[1]]
```

```{r}
ggsave("plots/proteomics-analysis/roc-curves.pdf",
       ts_roc, device = "pdf")
box_roc <- grid.arrange(ts_boxplot, lasso_boxplot, ts_roc, ncol = 3)
ggsave("plots/proteomics-analysis/box_roc.pdf",
       box_roc, device = "pdf",
       height = 2,
       width = 6)
ggsave("plots/proteomics-analysis/box_roc.eps",
       box_roc, device = "eps",
       height = 2,
       width = 6)
```

## Inference

We demonstrate the conditional test that the log-ratio model is appropriate.

```{r}
source("si.R")


# extract unique entries from z so that we have independent data points
z_full <- data.frame(rbind(z, zval))
y_full <- c(y, yval)
patid_full <- c(patid, max(patid) + patidval)
z_full$patid <- patid_full
z_full$y <- y_full
z_unique <- z_full %>% distinct(patid, .keep_all = TRUE)
z_final <- as.matrix(z_unique[,1:53])
y_final <- z_unique$y
n <- 54

set.seed(2)
# find appropriate lambda value
# we use noise level .25, which is an overestimate for 0-1 data
cv_fit <- cv.glmnet(z_final, y_final, standardize = F)
lambda = cv_fit$lambda.1se * n
gfit = glmnet(z_final, y_final, standardize=F, thresh = 10^-8)
b = coef(gfit, s=lambda/n, exact=TRUE)[-1]
out = fixedLassoInf(z_final, y_final,b,lambda,sigma=sqrt(noise_estimate),sumtest=T)
out$pv[1]
```